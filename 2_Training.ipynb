{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train\n",
    "---\n",
    "\n",
    "In this notebook, Will train the CNN-RNN model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  \n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.69s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 663/414113 [00:00<01:02, 6626.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:52<00:00, 7939.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 6        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 10            # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the  Model\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/12942], Loss: 4.5089, Perplexity: 90.8187\n",
      "Epoch [1/10], Step [200/12942], Loss: 3.7137, Perplexity: 41.00571\n",
      "Epoch [1/10], Step [300/12942], Loss: 3.4036, Perplexity: 30.0713\n",
      "Epoch [1/10], Step [400/12942], Loss: 3.7827, Perplexity: 43.9331\n",
      "Epoch [1/10], Step [500/12942], Loss: 3.3950, Perplexity: 29.8136\n",
      "Epoch [1/10], Step [600/12942], Loss: 3.7504, Perplexity: 42.5375\n",
      "Epoch [1/10], Step [700/12942], Loss: 3.4433, Perplexity: 31.2898\n",
      "Epoch [1/10], Step [800/12942], Loss: 3.7652, Perplexity: 43.1715\n",
      "Epoch [1/10], Step [900/12942], Loss: 3.4139, Perplexity: 30.3829\n",
      "Epoch [1/10], Step [1000/12942], Loss: 3.7258, Perplexity: 41.5041\n",
      "Epoch [1/10], Step [1100/12942], Loss: 2.8036, Perplexity: 16.5033\n",
      "Epoch [1/10], Step [1200/12942], Loss: 3.1838, Perplexity: 24.1387\n",
      "Epoch [1/10], Step [1300/12942], Loss: 3.6778, Perplexity: 39.5576\n",
      "Epoch [1/10], Step [1400/12942], Loss: 2.9622, Perplexity: 19.3411\n",
      "Epoch [1/10], Step [1500/12942], Loss: 3.2180, Perplexity: 24.9791\n",
      "Epoch [1/10], Step [1600/12942], Loss: 3.0700, Perplexity: 21.5429\n",
      "Epoch [1/10], Step [1700/12942], Loss: 3.0012, Perplexity: 20.10968\n",
      "Epoch [1/10], Step [1800/12942], Loss: 2.7254, Perplexity: 15.2624\n",
      "Epoch [1/10], Step [1900/12942], Loss: 2.9445, Perplexity: 19.0019\n",
      "Epoch [1/10], Step [2000/12942], Loss: 2.5465, Perplexity: 12.7619\n",
      "Epoch [1/10], Step [2100/12942], Loss: 2.8255, Perplexity: 16.8689\n",
      "Epoch [1/10], Step [2200/12942], Loss: 2.7112, Perplexity: 15.0479\n",
      "Epoch [1/10], Step [2300/12942], Loss: 3.0649, Perplexity: 21.4314\n",
      "Epoch [1/10], Step [2400/12942], Loss: 2.8730, Perplexity: 17.6901\n",
      "Epoch [1/10], Step [2500/12942], Loss: 2.5578, Perplexity: 12.9068\n",
      "Epoch [1/10], Step [2600/12942], Loss: 2.9540, Perplexity: 19.1823\n",
      "Epoch [1/10], Step [2700/12942], Loss: 2.6390, Perplexity: 13.9992\n",
      "Epoch [1/10], Step [2800/12942], Loss: 2.7270, Perplexity: 15.2877\n",
      "Epoch [1/10], Step [2900/12942], Loss: 2.4105, Perplexity: 11.1390\n",
      "Epoch [1/10], Step [3000/12942], Loss: 3.0137, Perplexity: 20.3619\n",
      "Epoch [1/10], Step [3100/12942], Loss: 2.6686, Perplexity: 14.4195\n",
      "Epoch [1/10], Step [3200/12942], Loss: 2.7856, Perplexity: 16.2096\n",
      "Epoch [1/10], Step [3300/12942], Loss: 2.3947, Perplexity: 10.9649\n",
      "Epoch [1/10], Step [3400/12942], Loss: 2.4667, Perplexity: 11.7831\n",
      "Epoch [1/10], Step [3500/12942], Loss: 2.4371, Perplexity: 11.43968\n",
      "Epoch [1/10], Step [3600/12942], Loss: 2.4243, Perplexity: 11.2940\n",
      "Epoch [1/10], Step [3700/12942], Loss: 2.6062, Perplexity: 13.5472\n",
      "Epoch [1/10], Step [3800/12942], Loss: 2.5231, Perplexity: 12.4677\n",
      "Epoch [1/10], Step [3900/12942], Loss: 2.5492, Perplexity: 12.7975\n",
      "Epoch [1/10], Step [4000/12942], Loss: 2.5331, Perplexity: 12.5925\n",
      "Epoch [1/10], Step [4100/12942], Loss: 2.3719, Perplexity: 10.7179\n",
      "Epoch [1/10], Step [4200/12942], Loss: 2.9181, Perplexity: 18.5066\n",
      "Epoch [1/10], Step [4300/12942], Loss: 2.7370, Perplexity: 15.4407\n",
      "Epoch [1/10], Step [4400/12942], Loss: 2.4692, Perplexity: 11.8130\n",
      "Epoch [1/10], Step [4500/12942], Loss: 2.1203, Perplexity: 8.334015\n",
      "Epoch [1/10], Step [4600/12942], Loss: 2.5289, Perplexity: 12.5391\n",
      "Epoch [1/10], Step [4700/12942], Loss: 2.7821, Perplexity: 16.1531\n",
      "Epoch [1/10], Step [4800/12942], Loss: 2.5367, Perplexity: 12.6381\n",
      "Epoch [1/10], Step [4900/12942], Loss: 2.5217, Perplexity: 12.4494\n",
      "Epoch [1/10], Step [5000/12942], Loss: 2.7770, Perplexity: 16.07086\n",
      "Epoch [1/10], Step [5100/12942], Loss: 2.5839, Perplexity: 13.24899\n",
      "Epoch [1/10], Step [5200/12942], Loss: 2.7771, Perplexity: 16.0719\n",
      "Epoch [1/10], Step [5300/12942], Loss: 2.4053, Perplexity: 11.0821\n",
      "Epoch [1/10], Step [5400/12942], Loss: 2.2835, Perplexity: 9.81066\n",
      "Epoch [1/10], Step [5500/12942], Loss: 2.5706, Perplexity: 13.0733\n",
      "Epoch [1/10], Step [5600/12942], Loss: 2.2308, Perplexity: 9.30700\n",
      "Epoch [1/10], Step [5700/12942], Loss: 2.4245, Perplexity: 11.2961\n",
      "Epoch [1/10], Step [5800/12942], Loss: 2.4192, Perplexity: 11.2366\n",
      "Epoch [1/10], Step [5900/12942], Loss: 2.2506, Perplexity: 9.49340\n",
      "Epoch [1/10], Step [6000/12942], Loss: 2.3031, Perplexity: 10.0053\n",
      "Epoch [1/10], Step [6100/12942], Loss: 2.7743, Perplexity: 16.0281\n",
      "Epoch [1/10], Step [6200/12942], Loss: 2.5135, Perplexity: 12.3482\n",
      "Epoch [1/10], Step [6300/12942], Loss: 2.6344, Perplexity: 13.9355\n",
      "Epoch [1/10], Step [6400/12942], Loss: 2.4350, Perplexity: 11.4160\n",
      "Epoch [1/10], Step [6500/12942], Loss: 2.6894, Perplexity: 14.7230\n",
      "Epoch [1/10], Step [6600/12942], Loss: 2.8831, Perplexity: 17.8695\n",
      "Epoch [1/10], Step [6700/12942], Loss: 2.1812, Perplexity: 8.85697\n",
      "Epoch [1/10], Step [6800/12942], Loss: 2.4263, Perplexity: 11.3171\n",
      "Epoch [1/10], Step [6900/12942], Loss: 2.5894, Perplexity: 13.3216\n",
      "Epoch [1/10], Step [7000/12942], Loss: 2.4218, Perplexity: 11.2660\n",
      "Epoch [1/10], Step [7100/12942], Loss: 2.6942, Perplexity: 14.7942\n",
      "Epoch [1/10], Step [7200/12942], Loss: 2.3770, Perplexity: 10.7720\n",
      "Epoch [1/10], Step [7300/12942], Loss: 2.2031, Perplexity: 9.05287\n",
      "Epoch [1/10], Step [7400/12942], Loss: 2.4547, Perplexity: 11.6431\n",
      "Epoch [1/10], Step [7500/12942], Loss: 2.4282, Perplexity: 11.3384\n",
      "Epoch [1/10], Step [7600/12942], Loss: 2.4855, Perplexity: 12.0068\n",
      "Epoch [1/10], Step [7700/12942], Loss: 2.6641, Perplexity: 14.3544\n",
      "Epoch [1/10], Step [7800/12942], Loss: 2.5222, Perplexity: 12.4562\n",
      "Epoch [1/10], Step [7900/12942], Loss: 2.9266, Perplexity: 18.6642\n",
      "Epoch [1/10], Step [8000/12942], Loss: 2.6992, Perplexity: 14.8673\n",
      "Epoch [1/10], Step [8100/12942], Loss: 2.3056, Perplexity: 10.0301\n",
      "Epoch [1/10], Step [8200/12942], Loss: 2.3188, Perplexity: 10.1633\n",
      "Epoch [1/10], Step [8300/12942], Loss: 2.5120, Perplexity: 12.3290\n",
      "Epoch [1/10], Step [8400/12942], Loss: 2.2050, Perplexity: 9.07033\n",
      "Epoch [1/10], Step [8500/12942], Loss: 2.6681, Perplexity: 14.4121\n",
      "Epoch [1/10], Step [8600/12942], Loss: 2.4286, Perplexity: 11.3425\n",
      "Epoch [1/10], Step [8700/12942], Loss: 2.6208, Perplexity: 13.7465\n",
      "Epoch [1/10], Step [8800/12942], Loss: 2.2077, Perplexity: 9.09495\n",
      "Epoch [1/10], Step [8900/12942], Loss: 2.5525, Perplexity: 12.8394\n",
      "Epoch [1/10], Step [9000/12942], Loss: 2.3374, Perplexity: 10.3541\n",
      "Epoch [1/10], Step [9100/12942], Loss: 2.5294, Perplexity: 12.5453\n",
      "Epoch [1/10], Step [9200/12942], Loss: 2.7382, Perplexity: 15.4594\n",
      "Epoch [1/10], Step [9300/12942], Loss: 2.3432, Perplexity: 10.4145\n",
      "Epoch [1/10], Step [9400/12942], Loss: 2.4750, Perplexity: 11.8817\n",
      "Epoch [1/10], Step [9500/12942], Loss: 3.1075, Perplexity: 22.36407\n",
      "Epoch [1/10], Step [9600/12942], Loss: 2.3797, Perplexity: 10.8013\n",
      "Epoch [1/10], Step [9700/12942], Loss: 2.5302, Perplexity: 12.5563\n",
      "Epoch [1/10], Step [9800/12942], Loss: 2.4255, Perplexity: 11.30761\n",
      "Epoch [1/10], Step [9900/12942], Loss: 2.9981, Perplexity: 20.0467\n",
      "Epoch [1/10], Step [10000/12942], Loss: 2.5477, Perplexity: 12.7778\n",
      "Epoch [1/10], Step [10100/12942], Loss: 2.5447, Perplexity: 12.7396\n",
      "Epoch [1/10], Step [10200/12942], Loss: 1.8026, Perplexity: 6.06537\n",
      "Epoch [1/10], Step [10300/12942], Loss: 2.4052, Perplexity: 11.0806\n",
      "Epoch [1/10], Step [10400/12942], Loss: 2.6183, Perplexity: 13.7129\n",
      "Epoch [1/10], Step [10500/12942], Loss: 2.0020, Perplexity: 7.40373\n",
      "Epoch [1/10], Step [10600/12942], Loss: 2.3084, Perplexity: 10.0587\n",
      "Epoch [1/10], Step [10700/12942], Loss: 2.8593, Perplexity: 17.4497\n",
      "Epoch [1/10], Step [10800/12942], Loss: 2.4651, Perplexity: 11.7651\n",
      "Epoch [1/10], Step [10900/12942], Loss: 2.1465, Perplexity: 8.55503\n",
      "Epoch [1/10], Step [11000/12942], Loss: 2.3530, Perplexity: 10.5173\n",
      "Epoch [1/10], Step [11100/12942], Loss: 2.5514, Perplexity: 12.8254\n",
      "Epoch [1/10], Step [11200/12942], Loss: 2.4797, Perplexity: 11.9377\n",
      "Epoch [1/10], Step [11300/12942], Loss: 2.7248, Perplexity: 15.2527\n",
      "Epoch [1/10], Step [11400/12942], Loss: 2.2709, Perplexity: 9.68817\n",
      "Epoch [1/10], Step [11500/12942], Loss: 2.1996, Perplexity: 9.02142\n",
      "Epoch [1/10], Step [11600/12942], Loss: 2.2830, Perplexity: 9.80612\n",
      "Epoch [1/10], Step [11700/12942], Loss: 2.1364, Perplexity: 8.46895\n",
      "Epoch [1/10], Step [11800/12942], Loss: 2.1569, Perplexity: 8.64423\n",
      "Epoch [1/10], Step [11900/12942], Loss: 2.5690, Perplexity: 13.0526\n",
      "Epoch [1/10], Step [12000/12942], Loss: 2.3775, Perplexity: 10.7783\n",
      "Epoch [1/10], Step [12100/12942], Loss: 2.5215, Perplexity: 12.4469\n",
      "Epoch [1/10], Step [12200/12942], Loss: 2.7067, Perplexity: 14.9799\n",
      "Epoch [1/10], Step [12300/12942], Loss: 2.4309, Perplexity: 11.3693\n",
      "Epoch [1/10], Step [12400/12942], Loss: 1.9213, Perplexity: 6.82957\n",
      "Epoch [1/10], Step [12500/12942], Loss: 2.2544, Perplexity: 9.52944\n",
      "Epoch [1/10], Step [12600/12942], Loss: 2.1479, Perplexity: 8.56711\n",
      "Epoch [1/10], Step [12700/12942], Loss: 2.3343, Perplexity: 10.3226\n",
      "Epoch [1/10], Step [12800/12942], Loss: 2.4271, Perplexity: 11.3256\n",
      "Epoch [1/10], Step [12900/12942], Loss: 2.3540, Perplexity: 10.5276\n",
      "Epoch [2/10], Step [100/12942], Loss: 2.3310, Perplexity: 10.288121\n",
      "Epoch [2/10], Step [200/12942], Loss: 2.3696, Perplexity: 10.6931\n",
      "Epoch [2/10], Step [300/12942], Loss: 2.3962, Perplexity: 10.9818\n",
      "Epoch [2/10], Step [400/12942], Loss: 2.1443, Perplexity: 8.53596\n",
      "Epoch [2/10], Step [500/12942], Loss: 2.2540, Perplexity: 9.52588\n",
      "Epoch [2/10], Step [600/12942], Loss: 2.0555, Perplexity: 7.81049\n",
      "Epoch [2/10], Step [700/12942], Loss: 2.2463, Perplexity: 9.45300\n",
      "Epoch [2/10], Step [800/12942], Loss: 2.3585, Perplexity: 10.5749\n",
      "Epoch [2/10], Step [900/12942], Loss: 1.9876, Perplexity: 7.29782\n",
      "Epoch [2/10], Step [1000/12942], Loss: 2.2153, Perplexity: 9.1640\n",
      "Epoch [2/10], Step [1100/12942], Loss: 2.3288, Perplexity: 10.2660\n",
      "Epoch [2/10], Step [1200/12942], Loss: 2.5430, Perplexity: 12.7180\n",
      "Epoch [2/10], Step [1300/12942], Loss: 2.4842, Perplexity: 11.9918\n",
      "Epoch [2/10], Step [1400/12942], Loss: 2.0745, Perplexity: 7.96098\n",
      "Epoch [2/10], Step [1500/12942], Loss: 2.1957, Perplexity: 8.98614\n",
      "Epoch [2/10], Step [1600/12942], Loss: 3.0139, Perplexity: 20.3667\n",
      "Epoch [2/10], Step [1700/12942], Loss: 3.1922, Perplexity: 24.3410\n",
      "Epoch [2/10], Step [1800/12942], Loss: 2.3325, Perplexity: 10.3036\n",
      "Epoch [2/10], Step [1900/12942], Loss: 2.2938, Perplexity: 9.91271\n",
      "Epoch [2/10], Step [2000/12942], Loss: 2.1822, Perplexity: 8.86615\n",
      "Epoch [2/10], Step [2100/12942], Loss: 2.1518, Perplexity: 8.60067\n",
      "Epoch [2/10], Step [2200/12942], Loss: 2.2249, Perplexity: 9.25280\n",
      "Epoch [2/10], Step [2300/12942], Loss: 2.1667, Perplexity: 8.72940\n",
      "Epoch [2/10], Step [2400/12942], Loss: 2.6124, Perplexity: 13.6315\n",
      "Epoch [2/10], Step [2500/12942], Loss: 2.3407, Perplexity: 10.3884\n",
      "Epoch [2/10], Step [2600/12942], Loss: 2.1216, Perplexity: 8.34437\n",
      "Epoch [2/10], Step [2700/12942], Loss: 2.2320, Perplexity: 9.31845\n",
      "Epoch [2/10], Step [2800/12942], Loss: 2.2779, Perplexity: 9.75646\n",
      "Epoch [2/10], Step [2900/12942], Loss: 2.4470, Perplexity: 11.5538\n",
      "Epoch [2/10], Step [3000/12942], Loss: 2.1401, Perplexity: 8.50035\n",
      "Epoch [2/10], Step [3100/12942], Loss: 2.1322, Perplexity: 8.43316\n",
      "Epoch [2/10], Step [3200/12942], Loss: 2.3917, Perplexity: 10.9323\n",
      "Epoch [2/10], Step [3300/12942], Loss: 2.0335, Perplexity: 7.64091\n",
      "Epoch [2/10], Step [3400/12942], Loss: 2.8171, Perplexity: 16.7283\n",
      "Epoch [2/10], Step [3500/12942], Loss: 2.1163, Perplexity: 8.30078\n",
      "Epoch [2/10], Step [3600/12942], Loss: 2.4274, Perplexity: 11.3295\n",
      "Epoch [2/10], Step [3700/12942], Loss: 2.1285, Perplexity: 8.40221\n",
      "Epoch [2/10], Step [3800/12942], Loss: 1.9686, Perplexity: 7.16073\n",
      "Epoch [2/10], Step [3900/12942], Loss: 2.0137, Perplexity: 7.49103\n",
      "Epoch [2/10], Step [4000/12942], Loss: 2.0327, Perplexity: 7.63478\n",
      "Epoch [2/10], Step [4100/12942], Loss: 2.0359, Perplexity: 7.65907\n",
      "Epoch [2/10], Step [4200/12942], Loss: 2.3570, Perplexity: 10.5590\n",
      "Epoch [2/10], Step [4300/12942], Loss: 2.0781, Perplexity: 7.98923\n",
      "Epoch [2/10], Step [4400/12942], Loss: 2.3028, Perplexity: 10.0021\n",
      "Epoch [2/10], Step [4500/12942], Loss: 2.0606, Perplexity: 7.85067\n",
      "Epoch [2/10], Step [4600/12942], Loss: 2.4150, Perplexity: 11.1901\n",
      "Epoch [2/10], Step [4700/12942], Loss: 2.6603, Perplexity: 14.3002\n",
      "Epoch [2/10], Step [4800/12942], Loss: 2.7408, Perplexity: 15.4990\n",
      "Epoch [2/10], Step [4900/12942], Loss: 2.2739, Perplexity: 9.71779\n",
      "Epoch [2/10], Step [5000/12942], Loss: 2.4637, Perplexity: 11.7485\n",
      "Epoch [2/10], Step [5100/12942], Loss: 2.6325, Perplexity: 13.9084\n",
      "Epoch [2/10], Step [5200/12942], Loss: 2.2935, Perplexity: 9.90911\n",
      "Epoch [2/10], Step [5300/12942], Loss: 2.2035, Perplexity: 9.05649\n",
      "Epoch [2/10], Step [5400/12942], Loss: 2.3860, Perplexity: 10.8696\n",
      "Epoch [2/10], Step [5500/12942], Loss: 2.2056, Perplexity: 9.07608\n",
      "Epoch [2/10], Step [5600/12942], Loss: 2.2003, Perplexity: 9.02742\n",
      "Epoch [2/10], Step [5700/12942], Loss: 2.0346, Perplexity: 7.64940\n",
      "Epoch [2/10], Step [5800/12942], Loss: 2.4983, Perplexity: 12.1623\n",
      "Epoch [2/10], Step [5900/12942], Loss: 1.8034, Perplexity: 6.07026\n",
      "Epoch [2/10], Step [6000/12942], Loss: 2.1042, Perplexity: 8.20094\n",
      "Epoch [2/10], Step [6100/12942], Loss: 2.1351, Perplexity: 8.45770\n",
      "Epoch [2/10], Step [6200/12942], Loss: 2.3567, Perplexity: 10.5560\n",
      "Epoch [2/10], Step [6300/12942], Loss: 2.0418, Perplexity: 7.70426\n",
      "Epoch [2/10], Step [6400/12942], Loss: 2.0802, Perplexity: 8.00629\n",
      "Epoch [2/10], Step [6500/12942], Loss: 2.3432, Perplexity: 10.4143\n",
      "Epoch [2/10], Step [6600/12942], Loss: 2.2628, Perplexity: 9.60981\n",
      "Epoch [2/10], Step [6700/12942], Loss: 2.1654, Perplexity: 8.71783\n",
      "Epoch [2/10], Step [6800/12942], Loss: 2.4598, Perplexity: 11.7027\n",
      "Epoch [2/10], Step [6900/12942], Loss: 2.2200, Perplexity: 9.20757\n",
      "Epoch [2/10], Step [7000/12942], Loss: 2.2796, Perplexity: 9.77264\n",
      "Epoch [2/10], Step [7100/12942], Loss: 2.4174, Perplexity: 11.2166\n",
      "Epoch [2/10], Step [7200/12942], Loss: 2.2679, Perplexity: 9.65938\n",
      "Epoch [2/10], Step [7300/12942], Loss: 2.3507, Perplexity: 10.4933\n",
      "Epoch [2/10], Step [7400/12942], Loss: 1.9930, Perplexity: 7.33773\n",
      "Epoch [2/10], Step [7500/12942], Loss: 2.0748, Perplexity: 7.96287\n",
      "Epoch [2/10], Step [7600/12942], Loss: 2.4015, Perplexity: 11.0393\n",
      "Epoch [2/10], Step [7700/12942], Loss: 2.5367, Perplexity: 12.6374\n",
      "Epoch [2/10], Step [7800/12942], Loss: 2.2591, Perplexity: 9.57493\n",
      "Epoch [2/10], Step [7900/12942], Loss: 2.3085, Perplexity: 10.0598\n",
      "Epoch [2/10], Step [8000/12942], Loss: 2.1868, Perplexity: 8.90654\n",
      "Epoch [2/10], Step [8100/12942], Loss: 2.3424, Perplexity: 10.4061\n",
      "Epoch [2/10], Step [8200/12942], Loss: 2.0944, Perplexity: 8.12060\n",
      "Epoch [2/10], Step [8300/12942], Loss: 2.0805, Perplexity: 8.00867\n",
      "Epoch [2/10], Step [8400/12942], Loss: 2.3266, Perplexity: 10.2428\n",
      "Epoch [2/10], Step [8500/12942], Loss: 2.2091, Perplexity: 9.10751\n",
      "Epoch [2/10], Step [8600/12942], Loss: 2.4058, Perplexity: 11.0876\n",
      "Epoch [2/10], Step [8700/12942], Loss: 2.4754, Perplexity: 11.8869\n",
      "Epoch [2/10], Step [8800/12942], Loss: 2.1524, Perplexity: 8.60596\n",
      "Epoch [2/10], Step [8900/12942], Loss: 2.2861, Perplexity: 9.83672\n",
      "Epoch [2/10], Step [9000/12942], Loss: 2.1199, Perplexity: 8.33019\n",
      "Epoch [2/10], Step [9100/12942], Loss: 2.4234, Perplexity: 11.2838\n",
      "Epoch [2/10], Step [9200/12942], Loss: 2.0920, Perplexity: 8.10149\n",
      "Epoch [2/10], Step [9300/12942], Loss: 2.1586, Perplexity: 8.65899\n",
      "Epoch [2/10], Step [9400/12942], Loss: 2.3945, Perplexity: 10.9627\n",
      "Epoch [2/10], Step [9500/12942], Loss: 2.7329, Perplexity: 15.3772\n",
      "Epoch [2/10], Step [9600/12942], Loss: 2.3479, Perplexity: 10.4633\n",
      "Epoch [2/10], Step [9700/12942], Loss: 2.4216, Perplexity: 11.2643\n",
      "Epoch [2/10], Step [9800/12942], Loss: 2.3894, Perplexity: 10.9065\n",
      "Epoch [2/10], Step [9900/12942], Loss: 2.0879, Perplexity: 8.06834\n",
      "Epoch [2/10], Step [10000/12942], Loss: 2.1504, Perplexity: 8.5882\n",
      "Epoch [2/10], Step [10100/12942], Loss: 2.3189, Perplexity: 10.1641\n",
      "Epoch [2/10], Step [10200/12942], Loss: 2.1200, Perplexity: 8.33116\n",
      "Epoch [2/10], Step [10300/12942], Loss: 2.1459, Perplexity: 8.55005\n",
      "Epoch [2/10], Step [10400/12942], Loss: 2.1542, Perplexity: 8.62117\n",
      "Epoch [2/10], Step [10500/12942], Loss: 2.4879, Perplexity: 12.0355\n",
      "Epoch [2/10], Step [10600/12942], Loss: 2.1651, Perplexity: 8.71560\n",
      "Epoch [2/10], Step [10700/12942], Loss: 2.1929, Perplexity: 8.96103\n",
      "Epoch [2/10], Step [10800/12942], Loss: 2.2382, Perplexity: 9.37641\n",
      "Epoch [2/10], Step [10900/12942], Loss: 1.8557, Perplexity: 6.39614\n",
      "Epoch [2/10], Step [11000/12942], Loss: 2.0040, Perplexity: 7.41888\n",
      "Epoch [2/10], Step [11100/12942], Loss: 2.5889, Perplexity: 13.3145\n",
      "Epoch [2/10], Step [11200/12942], Loss: 2.8393, Perplexity: 17.1040\n",
      "Epoch [2/10], Step [11300/12942], Loss: 2.1654, Perplexity: 8.71791\n",
      "Epoch [2/10], Step [11400/12942], Loss: 2.1052, Perplexity: 8.20844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [11500/12942], Loss: 2.1744, Perplexity: 8.79675\n",
      "Epoch [2/10], Step [11600/12942], Loss: 2.1598, Perplexity: 8.66960\n",
      "Epoch [2/10], Step [11700/12942], Loss: 2.3013, Perplexity: 9.98719\n",
      "Epoch [2/10], Step [11800/12942], Loss: 2.1057, Perplexity: 8.21259\n",
      "Epoch [2/10], Step [11900/12942], Loss: 2.3473, Perplexity: 10.4569\n",
      "Epoch [2/10], Step [12000/12942], Loss: 2.6425, Perplexity: 14.0482\n",
      "Epoch [2/10], Step [12100/12942], Loss: 2.2353, Perplexity: 9.34917\n",
      "Epoch [2/10], Step [12200/12942], Loss: 2.2098, Perplexity: 9.11355\n",
      "Epoch [2/10], Step [12300/12942], Loss: 2.2745, Perplexity: 9.72296\n",
      "Epoch [2/10], Step [12400/12942], Loss: 1.8171, Perplexity: 6.15377\n",
      "Epoch [2/10], Step [12500/12942], Loss: 2.0060, Perplexity: 7.43340\n",
      "Epoch [2/10], Step [12600/12942], Loss: 2.2640, Perplexity: 9.62149\n",
      "Epoch [2/10], Step [12700/12942], Loss: 2.0468, Perplexity: 7.74305\n",
      "Epoch [2/10], Step [12800/12942], Loss: 2.6293, Perplexity: 13.8639\n",
      "Epoch [2/10], Step [12900/12942], Loss: 2.2310, Perplexity: 9.30884\n",
      "Epoch [3/10], Step [100/12942], Loss: 3.3673, Perplexity: 28.999429\n",
      "Epoch [3/10], Step [200/12942], Loss: 2.0819, Perplexity: 8.02008\n",
      "Epoch [3/10], Step [300/12942], Loss: 2.5746, Perplexity: 13.1259\n",
      "Epoch [3/10], Step [400/12942], Loss: 2.0947, Perplexity: 8.12271\n",
      "Epoch [3/10], Step [500/12942], Loss: 2.1024, Perplexity: 8.18582\n",
      "Epoch [3/10], Step [600/12942], Loss: 2.2667, Perplexity: 9.64732\n",
      "Epoch [3/10], Step [700/12942], Loss: 2.3310, Perplexity: 10.2886\n",
      "Epoch [3/10], Step [800/12942], Loss: 3.0079, Perplexity: 20.2453\n",
      "Epoch [3/10], Step [900/12942], Loss: 2.2399, Perplexity: 9.39222\n",
      "Epoch [3/10], Step [1000/12942], Loss: 2.6098, Perplexity: 13.5963\n",
      "Epoch [3/10], Step [1100/12942], Loss: 2.3351, Perplexity: 10.3305\n",
      "Epoch [3/10], Step [1200/12942], Loss: 2.0644, Perplexity: 7.88033\n",
      "Epoch [3/10], Step [1300/12942], Loss: 2.0255, Perplexity: 7.57978\n",
      "Epoch [3/10], Step [1400/12942], Loss: 2.0105, Perplexity: 7.46722\n",
      "Epoch [3/10], Step [1500/12942], Loss: 2.1288, Perplexity: 8.40442\n",
      "Epoch [3/10], Step [1600/12942], Loss: 2.0635, Perplexity: 7.87368\n",
      "Epoch [3/10], Step [1700/12942], Loss: 2.4451, Perplexity: 11.5319\n",
      "Epoch [3/10], Step [1800/12942], Loss: 2.1604, Perplexity: 8.67506\n",
      "Epoch [3/10], Step [1900/12942], Loss: 2.3225, Perplexity: 10.2008\n",
      "Epoch [3/10], Step [2000/12942], Loss: 2.1514, Perplexity: 8.59722\n",
      "Epoch [3/10], Step [2100/12942], Loss: 1.9557, Perplexity: 7.06865\n",
      "Epoch [3/10], Step [2200/12942], Loss: 2.5938, Perplexity: 13.3803\n",
      "Epoch [3/10], Step [2300/12942], Loss: 2.7096, Perplexity: 15.0232\n",
      "Epoch [3/10], Step [2400/12942], Loss: 2.1335, Perplexity: 8.44411\n",
      "Epoch [3/10], Step [2500/12942], Loss: 1.9733, Perplexity: 7.19476\n",
      "Epoch [3/10], Step [2600/12942], Loss: 2.3342, Perplexity: 10.3209\n",
      "Epoch [3/10], Step [2700/12942], Loss: 2.2849, Perplexity: 9.82462\n",
      "Epoch [3/10], Step [2800/12942], Loss: 2.0869, Perplexity: 8.06014\n",
      "Epoch [3/10], Step [2900/12942], Loss: 2.0685, Perplexity: 7.91313\n",
      "Epoch [3/10], Step [3000/12942], Loss: 2.0827, Perplexity: 8.02591\n",
      "Epoch [3/10], Step [3100/12942], Loss: 2.1995, Perplexity: 9.02014\n",
      "Epoch [3/10], Step [3200/12942], Loss: 2.2006, Perplexity: 9.03075\n",
      "Epoch [3/10], Step [3300/12942], Loss: 2.3962, Perplexity: 10.9813\n",
      "Epoch [3/10], Step [3400/12942], Loss: 2.0412, Perplexity: 7.69984\n",
      "Epoch [3/10], Step [3500/12942], Loss: 2.1970, Perplexity: 8.99782\n",
      "Epoch [3/10], Step [3600/12942], Loss: 2.0862, Perplexity: 8.05447\n",
      "Epoch [3/10], Step [3700/12942], Loss: 2.2471, Perplexity: 9.46069\n",
      "Epoch [3/10], Step [3800/12942], Loss: 2.1899, Perplexity: 8.93390\n",
      "Epoch [3/10], Step [3900/12942], Loss: 2.2429, Perplexity: 9.42093\n",
      "Epoch [3/10], Step [4000/12942], Loss: 2.1828, Perplexity: 8.87074\n",
      "Epoch [3/10], Step [4100/12942], Loss: 2.3256, Perplexity: 10.2326\n",
      "Epoch [3/10], Step [4200/12942], Loss: 2.0349, Perplexity: 7.65117\n",
      "Epoch [3/10], Step [4300/12942], Loss: 2.0822, Perplexity: 8.02231\n",
      "Epoch [3/10], Step [4400/12942], Loss: 1.9711, Perplexity: 7.17843\n",
      "Epoch [3/10], Step [4500/12942], Loss: 2.3079, Perplexity: 10.0536\n",
      "Epoch [3/10], Step [4600/12942], Loss: 2.0938, Perplexity: 8.11550\n",
      "Epoch [3/10], Step [4700/12942], Loss: 2.8181, Perplexity: 16.7453\n",
      "Epoch [3/10], Step [4800/12942], Loss: 2.1199, Perplexity: 8.32990\n",
      "Epoch [3/10], Step [4900/12942], Loss: 2.1720, Perplexity: 8.77556\n",
      "Epoch [3/10], Step [5000/12942], Loss: 1.9763, Perplexity: 7.21601\n",
      "Epoch [3/10], Step [5100/12942], Loss: 2.2088, Perplexity: 9.10453\n",
      "Epoch [3/10], Step [5200/12942], Loss: 1.9758, Perplexity: 7.21228\n",
      "Epoch [3/10], Step [5300/12942], Loss: 2.0782, Perplexity: 7.99027\n",
      "Epoch [3/10], Step [5400/12942], Loss: 2.6644, Perplexity: 14.3599\n",
      "Epoch [3/10], Step [5500/12942], Loss: 2.1035, Perplexity: 8.19492\n",
      "Epoch [3/10], Step [5600/12942], Loss: 2.0793, Perplexity: 7.99923\n",
      "Epoch [3/10], Step [5700/12942], Loss: 2.0763, Perplexity: 7.97497\n",
      "Epoch [3/10], Step [5800/12942], Loss: 3.0797, Perplexity: 21.7516\n",
      "Epoch [3/10], Step [5900/12942], Loss: 2.2598, Perplexity: 9.58078\n",
      "Epoch [3/10], Step [6000/12942], Loss: 2.7587, Perplexity: 15.7798\n",
      "Epoch [3/10], Step [6100/12942], Loss: 2.1690, Perplexity: 8.74951\n",
      "Epoch [3/10], Step [6200/12942], Loss: 2.3363, Perplexity: 10.3431\n",
      "Epoch [3/10], Step [6300/12942], Loss: 2.0994, Perplexity: 8.16148\n",
      "Epoch [3/10], Step [6400/12942], Loss: 1.9947, Perplexity: 7.34988\n",
      "Epoch [3/10], Step [6500/12942], Loss: 2.7239, Perplexity: 15.2394\n",
      "Epoch [3/10], Step [6600/12942], Loss: 2.1592, Perplexity: 8.66433\n",
      "Epoch [3/10], Step [6700/12942], Loss: 2.1495, Perplexity: 8.58028\n",
      "Epoch [3/10], Step [6800/12942], Loss: 2.0227, Perplexity: 7.55849\n",
      "Epoch [3/10], Step [6900/12942], Loss: 2.3317, Perplexity: 10.2958\n",
      "Epoch [3/10], Step [7000/12942], Loss: 2.2651, Perplexity: 9.63210\n",
      "Epoch [3/10], Step [7100/12942], Loss: 2.2463, Perplexity: 9.45247\n",
      "Epoch [3/10], Step [7200/12942], Loss: 2.0914, Perplexity: 8.09666\n",
      "Epoch [3/10], Step [7300/12942], Loss: 2.3800, Perplexity: 10.8049\n",
      "Epoch [3/10], Step [7400/12942], Loss: 2.7117, Perplexity: 15.0550\n",
      "Epoch [3/10], Step [7500/12942], Loss: 2.0919, Perplexity: 8.10058\n",
      "Epoch [3/10], Step [7600/12942], Loss: 2.2443, Perplexity: 9.43367\n",
      "Epoch [3/10], Step [7700/12942], Loss: 2.1971, Perplexity: 8.99860\n",
      "Epoch [3/10], Step [7800/12942], Loss: 1.9878, Perplexity: 7.29979\n",
      "Epoch [3/10], Step [7900/12942], Loss: 2.5470, Perplexity: 12.7684\n",
      "Epoch [3/10], Step [8000/12942], Loss: 2.2118, Perplexity: 9.13262\n",
      "Epoch [3/10], Step [8100/12942], Loss: 2.2533, Perplexity: 9.51869\n",
      "Epoch [3/10], Step [8200/12942], Loss: 1.9366, Perplexity: 6.93540\n",
      "Epoch [3/10], Step [8300/12942], Loss: 2.0651, Perplexity: 7.88639\n",
      "Epoch [3/10], Step [8400/12942], Loss: 2.2669, Perplexity: 9.64910\n",
      "Epoch [3/10], Step [8500/12942], Loss: 2.0536, Perplexity: 7.79599\n",
      "Epoch [3/10], Step [8600/12942], Loss: 2.1147, Perplexity: 8.28724\n",
      "Epoch [3/10], Step [8700/12942], Loss: 2.1565, Perplexity: 8.64118\n",
      "Epoch [3/10], Step [8800/12942], Loss: 2.0457, Perplexity: 7.73448\n",
      "Epoch [3/10], Step [8900/12942], Loss: 2.1030, Perplexity: 8.19033\n",
      "Epoch [3/10], Step [9000/12942], Loss: 2.0104, Perplexity: 7.46602\n",
      "Epoch [3/10], Step [9100/12942], Loss: 2.1701, Perplexity: 8.75890\n",
      "Epoch [3/10], Step [9200/12942], Loss: 2.1574, Perplexity: 8.64904\n",
      "Epoch [3/10], Step [9300/12942], Loss: 2.0588, Perplexity: 7.83664\n",
      "Epoch [3/10], Step [9400/12942], Loss: 2.2138, Perplexity: 9.15025\n",
      "Epoch [3/10], Step [9500/12942], Loss: 2.4992, Perplexity: 12.1733\n",
      "Epoch [3/10], Step [9600/12942], Loss: 2.0943, Perplexity: 8.12018\n",
      "Epoch [3/10], Step [9700/12942], Loss: 2.1539, Perplexity: 8.61817\n",
      "Epoch [3/10], Step [9800/12942], Loss: 2.2559, Perplexity: 9.54359\n",
      "Epoch [3/10], Step [9900/12942], Loss: 2.2304, Perplexity: 9.30388\n",
      "Epoch [3/10], Step [10000/12942], Loss: 2.4476, Perplexity: 11.5605\n",
      "Epoch [3/10], Step [10100/12942], Loss: 2.0522, Perplexity: 7.78504\n",
      "Epoch [3/10], Step [10200/12942], Loss: 2.1299, Perplexity: 8.41385\n",
      "Epoch [3/10], Step [10300/12942], Loss: 2.8017, Perplexity: 16.4732\n",
      "Epoch [3/10], Step [10400/12942], Loss: 2.2785, Perplexity: 9.76222\n",
      "Epoch [3/10], Step [10500/12942], Loss: 1.9429, Perplexity: 6.978867\n",
      "Epoch [3/10], Step [10600/12942], Loss: 2.0255, Perplexity: 7.57962\n",
      "Epoch [3/10], Step [10700/12942], Loss: 2.1915, Perplexity: 8.94880\n",
      "Epoch [3/10], Step [10800/12942], Loss: 1.7695, Perplexity: 5.86794\n",
      "Epoch [3/10], Step [10900/12942], Loss: 1.9942, Perplexity: 7.34651\n",
      "Epoch [3/10], Step [11000/12942], Loss: 1.9295, Perplexity: 6.88624\n",
      "Epoch [3/10], Step [11100/12942], Loss: 2.0743, Perplexity: 7.95927\n",
      "Epoch [3/10], Step [11200/12942], Loss: 1.8882, Perplexity: 6.60775\n",
      "Epoch [3/10], Step [11300/12942], Loss: 2.0823, Perplexity: 8.02273\n",
      "Epoch [3/10], Step [11400/12942], Loss: 2.1123, Perplexity: 8.26757\n",
      "Epoch [3/10], Step [11500/12942], Loss: 1.9840, Perplexity: 7.27213\n",
      "Epoch [3/10], Step [11600/12942], Loss: 2.1099, Perplexity: 8.24743\n",
      "Epoch [3/10], Step [11700/12942], Loss: 2.3410, Perplexity: 10.3912\n",
      "Epoch [3/10], Step [11800/12942], Loss: 2.0306, Perplexity: 7.61888\n",
      "Epoch [3/10], Step [11900/12942], Loss: 2.2700, Perplexity: 9.67964\n",
      "Epoch [3/10], Step [12000/12942], Loss: 2.4038, Perplexity: 11.0655\n",
      "Epoch [3/10], Step [12100/12942], Loss: 2.0754, Perplexity: 7.96796\n",
      "Epoch [3/10], Step [12200/12942], Loss: 1.9141, Perplexity: 6.78110\n",
      "Epoch [3/10], Step [12300/12942], Loss: 2.2624, Perplexity: 9.60644\n",
      "Epoch [3/10], Step [12400/12942], Loss: 2.1939, Perplexity: 8.97011\n",
      "Epoch [3/10], Step [12500/12942], Loss: 2.2475, Perplexity: 9.46448\n",
      "Epoch [3/10], Step [12600/12942], Loss: 2.0583, Perplexity: 7.83241\n",
      "Epoch [3/10], Step [12700/12942], Loss: 2.3135, Perplexity: 10.1100\n",
      "Epoch [3/10], Step [12800/12942], Loss: 1.9912, Perplexity: 7.32420\n",
      "Epoch [3/10], Step [12900/12942], Loss: 2.2364, Perplexity: 9.35964\n",
      "Epoch [4/10], Step [100/12942], Loss: 2.2480, Perplexity: 9.4684385\n",
      "Epoch [4/10], Step [200/12942], Loss: 1.9787, Perplexity: 7.23315\n",
      "Epoch [4/10], Step [300/12942], Loss: 2.2508, Perplexity: 9.49527\n",
      "Epoch [4/10], Step [400/12942], Loss: 2.2002, Perplexity: 9.02681\n",
      "Epoch [4/10], Step [500/12942], Loss: 2.1653, Perplexity: 8.71720\n",
      "Epoch [4/10], Step [600/12942], Loss: 1.9569, Perplexity: 7.07760\n",
      "Epoch [4/10], Step [700/12942], Loss: 2.3157, Perplexity: 10.1323\n",
      "Epoch [4/10], Step [800/12942], Loss: 2.2848, Perplexity: 9.82373\n",
      "Epoch [4/10], Step [900/12942], Loss: 2.3179, Perplexity: 10.1545\n",
      "Epoch [4/10], Step [1000/12942], Loss: 2.7340, Perplexity: 15.3944\n",
      "Epoch [4/10], Step [1100/12942], Loss: 1.9402, Perplexity: 6.96035\n",
      "Epoch [4/10], Step [1200/12942], Loss: 2.3277, Perplexity: 10.2538\n",
      "Epoch [4/10], Step [1300/12942], Loss: 2.0720, Perplexity: 7.94054\n",
      "Epoch [4/10], Step [1400/12942], Loss: 2.3899, Perplexity: 10.9129\n",
      "Epoch [4/10], Step [1500/12942], Loss: 2.0222, Perplexity: 7.55515\n",
      "Epoch [4/10], Step [1600/12942], Loss: 2.2598, Perplexity: 9.58151\n",
      "Epoch [4/10], Step [1700/12942], Loss: 2.0311, Perplexity: 7.62224\n",
      "Epoch [4/10], Step [1800/12942], Loss: 2.1567, Perplexity: 8.64276\n",
      "Epoch [4/10], Step [1900/12942], Loss: 2.0644, Perplexity: 7.88075\n",
      "Epoch [4/10], Step [2000/12942], Loss: 2.3320, Perplexity: 10.2981\n",
      "Epoch [4/10], Step [2100/12942], Loss: 2.1598, Perplexity: 8.66988\n",
      "Epoch [4/10], Step [2200/12942], Loss: 1.8717, Perplexity: 6.49935\n",
      "Epoch [4/10], Step [2300/12942], Loss: 2.4488, Perplexity: 11.5739\n",
      "Epoch [4/10], Step [2400/12942], Loss: 2.2598, Perplexity: 9.58117\n",
      "Epoch [4/10], Step [2500/12942], Loss: 2.1407, Perplexity: 8.50533\n",
      "Epoch [4/10], Step [2600/12942], Loss: 2.3085, Perplexity: 10.0589\n",
      "Epoch [4/10], Step [2700/12942], Loss: 1.9084, Perplexity: 6.74242\n",
      "Epoch [4/10], Step [2800/12942], Loss: 2.2340, Perplexity: 9.33686\n",
      "Epoch [4/10], Step [2900/12942], Loss: 1.9470, Perplexity: 7.00798\n",
      "Epoch [4/10], Step [3000/12942], Loss: 2.1463, Perplexity: 8.55309\n",
      "Epoch [4/10], Step [3100/12942], Loss: 2.0822, Perplexity: 8.02243\n",
      "Epoch [4/10], Step [3200/12942], Loss: 1.7445, Perplexity: 5.72301\n",
      "Epoch [4/10], Step [3300/12942], Loss: 2.3591, Perplexity: 10.5818\n",
      "Epoch [4/10], Step [3400/12942], Loss: 1.8997, Perplexity: 6.68398\n",
      "Epoch [4/10], Step [3500/12942], Loss: 2.4206, Perplexity: 11.2526\n",
      "Epoch [4/10], Step [3600/12942], Loss: 2.1052, Perplexity: 8.20900\n",
      "Epoch [4/10], Step [3700/12942], Loss: 2.6011, Perplexity: 13.4780\n",
      "Epoch [4/10], Step [3800/12942], Loss: 2.2060, Perplexity: 9.07969\n",
      "Epoch [4/10], Step [3900/12942], Loss: 1.9195, Perplexity: 6.81746\n",
      "Epoch [4/10], Step [4000/12942], Loss: 2.0257, Perplexity: 7.58143\n",
      "Epoch [4/10], Step [4100/12942], Loss: 2.0352, Perplexity: 7.65353\n",
      "Epoch [4/10], Step [4200/12942], Loss: 2.5718, Perplexity: 13.0891\n",
      "Epoch [4/10], Step [4300/12942], Loss: 2.2755, Perplexity: 9.73278\n",
      "Epoch [4/10], Step [4400/12942], Loss: 2.0877, Perplexity: 8.06666\n",
      "Epoch [4/10], Step [4500/12942], Loss: 2.0466, Perplexity: 7.74137\n",
      "Epoch [4/10], Step [4600/12942], Loss: 2.2102, Perplexity: 9.11721\n",
      "Epoch [4/10], Step [4700/12942], Loss: 2.1010, Perplexity: 8.17418\n",
      "Epoch [4/10], Step [4800/12942], Loss: 1.9587, Perplexity: 7.09033\n",
      "Epoch [4/10], Step [4900/12942], Loss: 1.9635, Perplexity: 7.12401\n",
      "Epoch [4/10], Step [5000/12942], Loss: 1.9385, Perplexity: 6.94866\n",
      "Epoch [4/10], Step [5100/12942], Loss: 2.0042, Perplexity: 7.41998\n",
      "Epoch [4/10], Step [5200/12942], Loss: 2.3941, Perplexity: 10.9586\n",
      "Epoch [4/10], Step [5300/12942], Loss: 2.2870, Perplexity: 9.84542\n",
      "Epoch [4/10], Step [5400/12942], Loss: 2.0716, Perplexity: 7.93791\n",
      "Epoch [4/10], Step [5500/12942], Loss: 2.3514, Perplexity: 10.5006\n",
      "Epoch [4/10], Step [5600/12942], Loss: 1.9630, Perplexity: 7.12096\n",
      "Epoch [4/10], Step [5700/12942], Loss: 2.5224, Perplexity: 12.4586\n",
      "Epoch [4/10], Step [5800/12942], Loss: 2.1296, Perplexity: 8.41124\n",
      "Epoch [4/10], Step [5900/12942], Loss: 2.2722, Perplexity: 9.70105\n",
      "Epoch [4/10], Step [6000/12942], Loss: 1.8325, Perplexity: 6.24978\n",
      "Epoch [4/10], Step [6100/12942], Loss: 2.0912, Perplexity: 8.09492\n",
      "Epoch [4/10], Step [6200/12942], Loss: 1.9886, Perplexity: 7.30541\n",
      "Epoch [4/10], Step [6300/12942], Loss: 3.2137, Perplexity: 24.8713\n",
      "Epoch [4/10], Step [6400/12942], Loss: 2.2434, Perplexity: 9.42571\n",
      "Epoch [4/10], Step [6500/12942], Loss: 2.4225, Perplexity: 11.2738\n",
      "Epoch [4/10], Step [6600/12942], Loss: 2.5243, Perplexity: 12.4822\n",
      "Epoch [4/10], Step [6700/12942], Loss: 2.1615, Perplexity: 8.68393\n",
      "Epoch [4/10], Step [6800/12942], Loss: 1.9467, Perplexity: 7.00579\n",
      "Epoch [4/10], Step [6900/12942], Loss: 2.1707, Perplexity: 8.76437\n",
      "Epoch [4/10], Step [7000/12942], Loss: 1.9329, Perplexity: 6.90955\n",
      "Epoch [4/10], Step [7100/12942], Loss: 2.0351, Perplexity: 7.65285\n",
      "Epoch [4/10], Step [7200/12942], Loss: 2.0922, Perplexity: 8.10296\n",
      "Epoch [4/10], Step [7300/12942], Loss: 2.9119, Perplexity: 18.3917\n",
      "Epoch [4/10], Step [7400/12942], Loss: 2.2418, Perplexity: 9.40992\n",
      "Epoch [4/10], Step [7500/12942], Loss: 2.1577, Perplexity: 8.65133\n",
      "Epoch [4/10], Step [7600/12942], Loss: 2.3246, Perplexity: 10.2221\n",
      "Epoch [4/10], Step [7700/12942], Loss: 2.3081, Perplexity: 10.0550\n",
      "Epoch [4/10], Step [7800/12942], Loss: 1.9511, Perplexity: 7.03623\n",
      "Epoch [4/10], Step [7900/12942], Loss: 2.1063, Perplexity: 8.21808\n",
      "Epoch [4/10], Step [8000/12942], Loss: 2.1963, Perplexity: 8.99160\n",
      "Epoch [4/10], Step [8100/12942], Loss: 1.9346, Perplexity: 6.92161\n",
      "Epoch [4/10], Step [8200/12942], Loss: 1.9587, Perplexity: 7.08994\n",
      "Epoch [4/10], Step [8300/12942], Loss: 2.1303, Perplexity: 8.41702\n",
      "Epoch [4/10], Step [8400/12942], Loss: 2.1899, Perplexity: 8.93407\n",
      "Epoch [4/10], Step [8500/12942], Loss: 2.1208, Perplexity: 8.33827\n",
      "Epoch [4/10], Step [8600/12942], Loss: 1.9153, Perplexity: 6.78928\n",
      "Epoch [4/10], Step [8700/12942], Loss: 2.3327, Perplexity: 10.3054\n",
      "Epoch [4/10], Step [8800/12942], Loss: 2.2166, Perplexity: 9.17642\n",
      "Epoch [4/10], Step [8900/12942], Loss: 2.3083, Perplexity: 10.0572\n",
      "Epoch [4/10], Step [9000/12942], Loss: 2.8383, Perplexity: 17.0869\n",
      "Epoch [4/10], Step [9100/12942], Loss: 2.2534, Perplexity: 9.52025\n",
      "Epoch [4/10], Step [9200/12942], Loss: 2.1988, Perplexity: 9.01401\n",
      "Epoch [4/10], Step [9300/12942], Loss: 2.0993, Perplexity: 8.16034\n",
      "Epoch [4/10], Step [9400/12942], Loss: 1.8725, Perplexity: 6.50446\n",
      "Epoch [4/10], Step [9500/12942], Loss: 2.4381, Perplexity: 11.4508\n",
      "Epoch [4/10], Step [9600/12942], Loss: 2.3509, Perplexity: 10.4949\n",
      "Epoch [4/10], Step [9700/12942], Loss: 2.2299, Perplexity: 9.29891\n",
      "Epoch [4/10], Step [9800/12942], Loss: 2.2886, Perplexity: 9.86122\n",
      "Epoch [4/10], Step [9900/12942], Loss: 2.4735, Perplexity: 11.8641\n",
      "Epoch [4/10], Step [10000/12942], Loss: 2.1545, Perplexity: 8.6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [10100/12942], Loss: 2.0121, Perplexity: 7.47937\n",
      "Epoch [4/10], Step [10200/12942], Loss: 1.9931, Perplexity: 7.33856\n",
      "Epoch [4/10], Step [10300/12942], Loss: 1.9858, Perplexity: 7.28495\n",
      "Epoch [4/10], Step [10400/12942], Loss: 3.1465, Perplexity: 23.2546\n",
      "Epoch [4/10], Step [10500/12942], Loss: 1.8773, Perplexity: 6.53579\n",
      "Epoch [4/10], Step [10600/12942], Loss: 2.0325, Perplexity: 7.63292\n",
      "Epoch [4/10], Step [10700/12942], Loss: 2.1606, Perplexity: 8.67632\n",
      "Epoch [4/10], Step [10800/12942], Loss: 1.9516, Perplexity: 7.04002\n",
      "Epoch [4/10], Step [10900/12942], Loss: 1.9791, Perplexity: 7.23644\n",
      "Epoch [4/10], Step [11000/12942], Loss: 2.6141, Perplexity: 13.6549\n",
      "Epoch [4/10], Step [11100/12942], Loss: 1.8595, Perplexity: 6.42072\n",
      "Epoch [4/10], Step [11200/12942], Loss: 1.9440, Perplexity: 6.98677\n",
      "Epoch [4/10], Step [11300/12942], Loss: 1.9298, Perplexity: 6.88839\n",
      "Epoch [4/10], Step [11400/12942], Loss: 2.0280, Perplexity: 7.59899\n",
      "Epoch [4/10], Step [11500/12942], Loss: 2.0533, Perplexity: 7.79365\n",
      "Epoch [4/10], Step [11600/12942], Loss: 2.0667, Perplexity: 7.89840\n",
      "Epoch [4/10], Step [11700/12942], Loss: 2.4260, Perplexity: 11.3140\n",
      "Epoch [4/10], Step [11800/12942], Loss: 2.4514, Perplexity: 11.6047\n",
      "Epoch [4/10], Step [11900/12942], Loss: 2.4554, Perplexity: 11.6506\n",
      "Epoch [4/10], Step [12000/12942], Loss: 2.0210, Perplexity: 7.54597\n",
      "Epoch [4/10], Step [12100/12942], Loss: 1.9327, Perplexity: 6.90833\n",
      "Epoch [4/10], Step [12200/12942], Loss: 2.1040, Perplexity: 8.19939\n",
      "Epoch [4/10], Step [12300/12942], Loss: 2.1960, Perplexity: 8.98896\n",
      "Epoch [4/10], Step [12400/12942], Loss: 1.8583, Perplexity: 6.41309\n",
      "Epoch [4/10], Step [12500/12942], Loss: 2.0871, Perplexity: 8.06177\n",
      "Epoch [4/10], Step [12600/12942], Loss: 2.1360, Perplexity: 8.46586\n",
      "Epoch [4/10], Step [12700/12942], Loss: 2.2811, Perplexity: 9.78796\n",
      "Epoch [4/10], Step [12800/12942], Loss: 2.2600, Perplexity: 9.58270\n",
      "Epoch [4/10], Step [12900/12942], Loss: 1.8719, Perplexity: 6.50072\n",
      "Epoch [5/10], Step [100/12942], Loss: 2.3022, Perplexity: 9.9965991\n",
      "Epoch [5/10], Step [200/12942], Loss: 2.1863, Perplexity: 8.90267\n",
      "Epoch [5/10], Step [300/12942], Loss: 1.9658, Perplexity: 7.14034\n",
      "Epoch [5/10], Step [400/12942], Loss: 2.2349, Perplexity: 9.34567\n",
      "Epoch [5/10], Step [500/12942], Loss: 2.0613, Perplexity: 7.85613\n",
      "Epoch [5/10], Step [600/12942], Loss: 2.0971, Perplexity: 8.14238\n",
      "Epoch [5/10], Step [700/12942], Loss: 2.1105, Perplexity: 8.25219\n",
      "Epoch [5/10], Step [800/12942], Loss: 1.8783, Perplexity: 6.54243\n",
      "Epoch [5/10], Step [900/12942], Loss: 2.1378, Perplexity: 8.48062\n",
      "Epoch [5/10], Step [1000/12942], Loss: 2.1457, Perplexity: 8.5483\n",
      "Epoch [5/10], Step [1100/12942], Loss: 2.1542, Perplexity: 8.62096\n",
      "Epoch [5/10], Step [1200/12942], Loss: 2.1473, Perplexity: 8.56206\n",
      "Epoch [5/10], Step [1300/12942], Loss: 2.3985, Perplexity: 11.0062\n",
      "Epoch [5/10], Step [1400/12942], Loss: 2.9967, Perplexity: 20.0188\n",
      "Epoch [5/10], Step [1500/12942], Loss: 2.6287, Perplexity: 13.8557\n",
      "Epoch [5/10], Step [1600/12942], Loss: 2.1431, Perplexity: 8.52611\n",
      "Epoch [5/10], Step [1700/12942], Loss: 1.9979, Perplexity: 7.37340\n",
      "Epoch [5/10], Step [1800/12942], Loss: 2.1735, Perplexity: 8.78873\n",
      "Epoch [5/10], Step [1900/12942], Loss: 1.9876, Perplexity: 7.29806\n",
      "Epoch [5/10], Step [2000/12942], Loss: 2.0192, Perplexity: 7.53206\n",
      "Epoch [5/10], Step [2100/12942], Loss: 2.0081, Perplexity: 7.44918\n",
      "Epoch [5/10], Step [2200/12942], Loss: 2.0044, Perplexity: 7.42174\n",
      "Epoch [5/10], Step [2300/12942], Loss: 1.9619, Perplexity: 7.11316\n",
      "Epoch [5/10], Step [2400/12942], Loss: 2.2773, Perplexity: 9.75024\n",
      "Epoch [5/10], Step [2500/12942], Loss: 2.0438, Perplexity: 7.71976\n",
      "Epoch [5/10], Step [2600/12942], Loss: 2.2513, Perplexity: 9.50013\n",
      "Epoch [5/10], Step [2700/12942], Loss: 2.1198, Perplexity: 8.32913\n",
      "Epoch [5/10], Step [2800/12942], Loss: 2.2458, Perplexity: 9.44828\n",
      "Epoch [5/10], Step [2900/12942], Loss: 2.1927, Perplexity: 8.95948\n",
      "Epoch [5/10], Step [3000/12942], Loss: 2.3129, Perplexity: 10.1039\n",
      "Epoch [5/10], Step [3100/12942], Loss: 2.6647, Perplexity: 14.3633\n",
      "Epoch [5/10], Step [3200/12942], Loss: 1.9883, Perplexity: 7.30284\n",
      "Epoch [5/10], Step [3300/12942], Loss: 2.0065, Perplexity: 7.43709\n",
      "Epoch [5/10], Step [3400/12942], Loss: 2.4236, Perplexity: 11.2860\n",
      "Epoch [5/10], Step [3500/12942], Loss: 2.2466, Perplexity: 9.45551\n",
      "Epoch [5/10], Step [3600/12942], Loss: 2.5650, Perplexity: 13.0003\n",
      "Epoch [5/10], Step [3700/12942], Loss: 2.2466, Perplexity: 9.45528\n",
      "Epoch [5/10], Step [3800/12942], Loss: 1.9607, Perplexity: 7.10410\n",
      "Epoch [5/10], Step [3900/12942], Loss: 2.2634, Perplexity: 9.61543\n",
      "Epoch [5/10], Step [4000/12942], Loss: 2.2707, Perplexity: 9.68595\n",
      "Epoch [5/10], Step [4100/12942], Loss: 1.9958, Perplexity: 7.35789\n",
      "Epoch [5/10], Step [4200/12942], Loss: 1.8800, Perplexity: 6.55375\n",
      "Epoch [5/10], Step [4300/12942], Loss: 2.2412, Perplexity: 9.40466\n",
      "Epoch [5/10], Step [4400/12942], Loss: 2.4434, Perplexity: 11.5125\n",
      "Epoch [5/10], Step [4500/12942], Loss: 2.3578, Perplexity: 10.5681\n",
      "Epoch [5/10], Step [4600/12942], Loss: 2.1252, Perplexity: 8.37440\n",
      "Epoch [5/10], Step [4700/12942], Loss: 2.5576, Perplexity: 12.9047\n",
      "Epoch [5/10], Step [4800/12942], Loss: 2.3995, Perplexity: 11.0177\n",
      "Epoch [5/10], Step [4900/12942], Loss: 2.1600, Perplexity: 8.67105\n",
      "Epoch [5/10], Step [5000/12942], Loss: 2.1686, Perplexity: 8.74629\n",
      "Epoch [5/10], Step [5100/12942], Loss: 2.2978, Perplexity: 9.95221\n",
      "Epoch [5/10], Step [5200/12942], Loss: 1.9633, Perplexity: 7.12262\n",
      "Epoch [5/10], Step [5300/12942], Loss: 2.4495, Perplexity: 11.5829\n",
      "Epoch [5/10], Step [5400/12942], Loss: 2.1737, Perplexity: 8.79062\n",
      "Epoch [5/10], Step [5500/12942], Loss: 2.2053, Perplexity: 9.07313\n",
      "Epoch [5/10], Step [5600/12942], Loss: 2.3527, Perplexity: 10.5144\n",
      "Epoch [5/10], Step [5700/12942], Loss: 2.1136, Perplexity: 8.27762\n",
      "Epoch [5/10], Step [5800/12942], Loss: 2.0790, Perplexity: 7.99688\n",
      "Epoch [5/10], Step [5900/12942], Loss: 2.2591, Perplexity: 9.57449\n",
      "Epoch [5/10], Step [6000/12942], Loss: 1.8562, Perplexity: 6.39916\n",
      "Epoch [5/10], Step [6100/12942], Loss: 1.8806, Perplexity: 6.55743\n",
      "Epoch [5/10], Step [6200/12942], Loss: 2.1097, Perplexity: 8.24578\n",
      "Epoch [5/10], Step [6300/12942], Loss: 2.6549, Perplexity: 14.2230\n",
      "Epoch [5/10], Step [6400/12942], Loss: 2.0756, Perplexity: 7.96942\n",
      "Epoch [5/10], Step [6500/12942], Loss: 1.9728, Perplexity: 7.19085\n",
      "Epoch [5/10], Step [6600/12942], Loss: 1.8529, Perplexity: 6.37802\n",
      "Epoch [5/10], Step [6700/12942], Loss: 2.0717, Perplexity: 7.93811\n",
      "Epoch [5/10], Step [6800/12942], Loss: 2.1285, Perplexity: 8.40234\n",
      "Epoch [5/10], Step [6900/12942], Loss: 2.3210, Perplexity: 10.1858\n",
      "Epoch [5/10], Step [7000/12942], Loss: 2.4033, Perplexity: 11.0592\n",
      "Epoch [5/10], Step [7100/12942], Loss: 2.1060, Perplexity: 8.21531\n",
      "Epoch [5/10], Step [7200/12942], Loss: 2.2248, Perplexity: 9.25195\n",
      "Epoch [5/10], Step [7300/12942], Loss: 2.0738, Perplexity: 7.95538\n",
      "Epoch [5/10], Step [7400/12942], Loss: 2.3197, Perplexity: 10.1727\n",
      "Epoch [5/10], Step [7500/12942], Loss: 2.0812, Perplexity: 8.01440\n",
      "Epoch [5/10], Step [7600/12942], Loss: 2.3335, Perplexity: 10.3142\n",
      "Epoch [5/10], Step [7700/12942], Loss: 2.2163, Perplexity: 9.17359\n",
      "Epoch [5/10], Step [7800/12942], Loss: 2.3228, Perplexity: 10.2039\n",
      "Epoch [5/10], Step [7900/12942], Loss: 2.4693, Perplexity: 11.8144\n",
      "Epoch [5/10], Step [8000/12942], Loss: 1.9040, Perplexity: 6.71288\n",
      "Epoch [5/10], Step [8100/12942], Loss: 2.3475, Perplexity: 10.4597\n",
      "Epoch [5/10], Step [8200/12942], Loss: 1.8953, Perplexity: 6.65498\n",
      "Epoch [5/10], Step [8300/12942], Loss: 1.8864, Perplexity: 6.59569\n",
      "Epoch [5/10], Step [8400/12942], Loss: 2.2634, Perplexity: 9.61614\n",
      "Epoch [5/10], Step [8500/12942], Loss: 2.1347, Perplexity: 8.45462\n",
      "Epoch [5/10], Step [8600/12942], Loss: 2.4520, Perplexity: 11.6114\n",
      "Epoch [5/10], Step [8700/12942], Loss: 2.2999, Perplexity: 9.97355\n",
      "Epoch [5/10], Step [8800/12942], Loss: 1.7463, Perplexity: 5.73340\n",
      "Epoch [5/10], Step [8900/12942], Loss: 2.2237, Perplexity: 9.24172\n",
      "Epoch [5/10], Step [9000/12942], Loss: 1.9367, Perplexity: 6.93592\n",
      "Epoch [5/10], Step [9100/12942], Loss: 1.9692, Perplexity: 7.16510\n",
      "Epoch [5/10], Step [9200/12942], Loss: 2.4722, Perplexity: 11.8488\n",
      "Epoch [5/10], Step [9300/12942], Loss: 1.9831, Perplexity: 7.26546\n",
      "Epoch [5/10], Step [9400/12942], Loss: 2.3987, Perplexity: 11.0093\n",
      "Epoch [5/10], Step [9500/12942], Loss: 2.1428, Perplexity: 8.52293\n",
      "Epoch [5/10], Step [9600/12942], Loss: 1.9503, Perplexity: 7.03098\n",
      "Epoch [5/10], Step [9700/12942], Loss: 2.0332, Perplexity: 7.63863\n",
      "Epoch [5/10], Step [9800/12942], Loss: 1.9319, Perplexity: 6.90233\n",
      "Epoch [5/10], Step [9900/12942], Loss: 2.1162, Perplexity: 8.29939\n",
      "Epoch [5/10], Step [10000/12942], Loss: 2.0193, Perplexity: 7.5333\n",
      "Epoch [5/10], Step [10100/12942], Loss: 2.1820, Perplexity: 8.86416\n",
      "Epoch [5/10], Step [10200/12942], Loss: 2.3527, Perplexity: 10.5135\n",
      "Epoch [5/10], Step [10300/12942], Loss: 1.9897, Perplexity: 7.31324\n",
      "Epoch [5/10], Step [10400/12942], Loss: 2.0742, Perplexity: 7.95804\n",
      "Epoch [5/10], Step [10500/12942], Loss: 2.0966, Perplexity: 8.13833\n",
      "Epoch [5/10], Step [10600/12942], Loss: 2.4019, Perplexity: 11.0443\n",
      "Epoch [5/10], Step [10700/12942], Loss: 2.3257, Perplexity: 10.2335\n",
      "Epoch [5/10], Step [10800/12942], Loss: 2.0796, Perplexity: 8.001125\n",
      "Epoch [5/10], Step [10900/12942], Loss: 1.8088, Perplexity: 6.10293\n",
      "Epoch [5/10], Step [11000/12942], Loss: 1.9607, Perplexity: 7.10442\n",
      "Epoch [5/10], Step [11100/12942], Loss: 1.8060, Perplexity: 6.08601\n",
      "Epoch [5/10], Step [11200/12942], Loss: 2.2231, Perplexity: 9.23628\n",
      "Epoch [5/10], Step [11300/12942], Loss: 2.0720, Perplexity: 7.94102\n",
      "Epoch [5/10], Step [11400/12942], Loss: 1.9599, Perplexity: 7.09879\n",
      "Epoch [5/10], Step [11500/12942], Loss: 2.1795, Perplexity: 8.84221\n",
      "Epoch [5/10], Step [11600/12942], Loss: 2.1311, Perplexity: 8.42393\n",
      "Epoch [5/10], Step [11700/12942], Loss: 2.2504, Perplexity: 9.49130\n",
      "Epoch [5/10], Step [11800/12942], Loss: 2.3493, Perplexity: 10.4783\n",
      "Epoch [5/10], Step [11900/12942], Loss: 2.2114, Perplexity: 9.12810\n",
      "Epoch [5/10], Step [12000/12942], Loss: 2.0644, Perplexity: 7.88065\n",
      "Epoch [5/10], Step [12100/12942], Loss: 2.4792, Perplexity: 11.9315\n",
      "Epoch [5/10], Step [12200/12942], Loss: 2.2404, Perplexity: 9.39723\n",
      "Epoch [5/10], Step [12300/12942], Loss: 1.9416, Perplexity: 6.97003\n",
      "Epoch [5/10], Step [12400/12942], Loss: 2.0955, Perplexity: 8.12983\n",
      "Epoch [5/10], Step [12500/12942], Loss: 2.1322, Perplexity: 8.43318\n",
      "Epoch [5/10], Step [12600/12942], Loss: 2.2912, Perplexity: 9.88737\n",
      "Epoch [5/10], Step [12700/12942], Loss: 2.2124, Perplexity: 9.13808\n",
      "Epoch [5/10], Step [12800/12942], Loss: 1.9604, Perplexity: 7.10233\n",
      "Epoch [5/10], Step [12900/12942], Loss: 2.1795, Perplexity: 8.84150\n",
      "Epoch [6/10], Step [100/12942], Loss: 2.1688, Perplexity: 8.7482786\n",
      "Epoch [6/10], Step [200/12942], Loss: 2.0647, Perplexity: 7.88324\n",
      "Epoch [6/10], Step [300/12942], Loss: 2.0066, Perplexity: 7.43806\n",
      "Epoch [6/10], Step [400/12942], Loss: 2.0448, Perplexity: 7.72792\n",
      "Epoch [6/10], Step [500/12942], Loss: 2.0206, Perplexity: 7.54282\n",
      "Epoch [6/10], Step [600/12942], Loss: 2.3550, Perplexity: 10.5378\n",
      "Epoch [6/10], Step [700/12942], Loss: 1.8952, Perplexity: 6.65381\n",
      "Epoch [6/10], Step [800/12942], Loss: 1.9138, Perplexity: 6.77866\n",
      "Epoch [6/10], Step [900/12942], Loss: 2.4128, Perplexity: 11.1656\n",
      "Epoch [6/10], Step [1000/12942], Loss: 1.9637, Perplexity: 7.1259\n",
      "Epoch [6/10], Step [1100/12942], Loss: 1.8557, Perplexity: 6.39638\n",
      "Epoch [6/10], Step [1200/12942], Loss: 2.0187, Perplexity: 7.52857\n",
      "Epoch [6/10], Step [1300/12942], Loss: 2.1906, Perplexity: 8.94027\n",
      "Epoch [6/10], Step [1400/12942], Loss: 2.5084, Perplexity: 12.2854\n",
      "Epoch [6/10], Step [1500/12942], Loss: 2.3786, Perplexity: 10.7902\n",
      "Epoch [6/10], Step [1600/12942], Loss: 2.3665, Perplexity: 10.6602\n",
      "Epoch [6/10], Step [1700/12942], Loss: 2.2759, Perplexity: 9.73640\n",
      "Epoch [6/10], Step [1800/12942], Loss: 2.1476, Perplexity: 8.56417\n",
      "Epoch [6/10], Step [1900/12942], Loss: 2.4947, Perplexity: 12.1176\n",
      "Epoch [6/10], Step [2000/12942], Loss: 2.2287, Perplexity: 9.28796\n",
      "Epoch [6/10], Step [2100/12942], Loss: 2.1063, Perplexity: 8.21827\n",
      "Epoch [6/10], Step [2200/12942], Loss: 1.8769, Perplexity: 6.53357\n",
      "Epoch [6/10], Step [2300/12942], Loss: 2.2376, Perplexity: 9.37126\n",
      "Epoch [6/10], Step [2400/12942], Loss: 2.1256, Perplexity: 8.37801\n",
      "Epoch [6/10], Step [2500/12942], Loss: 2.1570, Perplexity: 8.64539\n",
      "Epoch [6/10], Step [2600/12942], Loss: 2.0887, Perplexity: 8.07475\n",
      "Epoch [6/10], Step [2700/12942], Loss: 2.2214, Perplexity: 9.22011\n",
      "Epoch [6/10], Step [2800/12942], Loss: 1.8190, Perplexity: 6.16579\n",
      "Epoch [6/10], Step [2900/12942], Loss: 2.1721, Perplexity: 8.77631\n",
      "Epoch [6/10], Step [3000/12942], Loss: 2.3397, Perplexity: 10.3783\n",
      "Epoch [6/10], Step [3100/12942], Loss: 2.1348, Perplexity: 8.45567\n",
      "Epoch [6/10], Step [3200/12942], Loss: 1.9897, Perplexity: 7.31326\n",
      "Epoch [6/10], Step [3300/12942], Loss: 2.1840, Perplexity: 8.88196\n",
      "Epoch [6/10], Step [3400/12942], Loss: 2.2922, Perplexity: 9.89643\n",
      "Epoch [6/10], Step [3500/12942], Loss: 2.3803, Perplexity: 10.8080\n",
      "Epoch [6/10], Step [3600/12942], Loss: 1.9067, Perplexity: 6.73071\n",
      "Epoch [6/10], Step [3700/12942], Loss: 2.0601, Perplexity: 7.84663\n",
      "Epoch [6/10], Step [3800/12942], Loss: 2.1379, Perplexity: 8.48183\n",
      "Epoch [6/10], Step [3900/12942], Loss: 2.1376, Perplexity: 8.47916\n",
      "Epoch [6/10], Step [4000/12942], Loss: 1.8981, Perplexity: 6.67354\n",
      "Epoch [6/10], Step [4100/12942], Loss: 1.9921, Perplexity: 7.33063\n",
      "Epoch [6/10], Step [4200/12942], Loss: 2.1811, Perplexity: 8.85626\n",
      "Epoch [6/10], Step [4300/12942], Loss: 2.7712, Perplexity: 15.9784\n",
      "Epoch [6/10], Step [4400/12942], Loss: 2.3259, Perplexity: 10.2358\n",
      "Epoch [6/10], Step [4500/12942], Loss: 2.2467, Perplexity: 9.45698\n",
      "Epoch [6/10], Step [4600/12942], Loss: 1.9123, Perplexity: 6.76881\n",
      "Epoch [6/10], Step [4700/12942], Loss: 2.4467, Perplexity: 11.5498\n",
      "Epoch [6/10], Step [4800/12942], Loss: 2.2026, Perplexity: 9.04869\n",
      "Epoch [6/10], Step [4900/12942], Loss: 2.2709, Perplexity: 9.68778\n",
      "Epoch [6/10], Step [5000/12942], Loss: 2.9837, Perplexity: 19.7616\n",
      "Epoch [6/10], Step [5100/12942], Loss: 1.9910, Perplexity: 7.32259\n",
      "Epoch [6/10], Step [5200/12942], Loss: 2.4412, Perplexity: 11.4872\n",
      "Epoch [6/10], Step [5300/12942], Loss: 2.1762, Perplexity: 8.81288\n",
      "Epoch [6/10], Step [5400/12942], Loss: 2.3195, Perplexity: 10.1705\n",
      "Epoch [6/10], Step [5500/12942], Loss: 1.9600, Perplexity: 7.09955\n",
      "Epoch [6/10], Step [5600/12942], Loss: 2.1980, Perplexity: 9.00726\n",
      "Epoch [6/10], Step [5700/12942], Loss: 2.1442, Perplexity: 8.53539\n",
      "Epoch [6/10], Step [5800/12942], Loss: 1.9477, Perplexity: 7.01282\n",
      "Epoch [6/10], Step [5900/12942], Loss: 1.9260, Perplexity: 6.86208\n",
      "Epoch [6/10], Step [6000/12942], Loss: 2.2834, Perplexity: 9.81045\n",
      "Epoch [6/10], Step [6100/12942], Loss: 1.9371, Perplexity: 6.93833\n",
      "Epoch [6/10], Step [6200/12942], Loss: 1.7046, Perplexity: 5.49924\n",
      "Epoch [6/10], Step [6300/12942], Loss: 2.5030, Perplexity: 12.2193\n",
      "Epoch [6/10], Step [6400/12942], Loss: 2.0931, Perplexity: 8.110123\n",
      "Epoch [6/10], Step [6500/12942], Loss: 2.0754, Perplexity: 7.96793\n",
      "Epoch [6/10], Step [6600/12942], Loss: 2.2542, Perplexity: 9.52743\n",
      "Epoch [6/10], Step [6700/12942], Loss: 1.9771, Perplexity: 7.22200\n",
      "Epoch [6/10], Step [6800/12942], Loss: 2.1463, Perplexity: 8.55301\n",
      "Epoch [6/10], Step [6900/12942], Loss: 2.1589, Perplexity: 8.66148\n",
      "Epoch [6/10], Step [7000/12942], Loss: 1.9608, Perplexity: 7.10496\n",
      "Epoch [6/10], Step [7100/12942], Loss: 2.3068, Perplexity: 10.0421\n",
      "Epoch [6/10], Step [7200/12942], Loss: 2.2362, Perplexity: 9.35767\n",
      "Epoch [6/10], Step [7300/12942], Loss: 2.0184, Perplexity: 7.52656\n",
      "Epoch [6/10], Step [7400/12942], Loss: 2.0733, Perplexity: 7.95095\n",
      "Epoch [6/10], Step [7500/12942], Loss: 2.1774, Perplexity: 8.82389\n",
      "Epoch [6/10], Step [7600/12942], Loss: 2.3053, Perplexity: 10.0270\n",
      "Epoch [6/10], Step [7700/12942], Loss: 2.2229, Perplexity: 9.23361\n",
      "Epoch [6/10], Step [7800/12942], Loss: 2.2557, Perplexity: 9.54244\n",
      "Epoch [6/10], Step [7900/12942], Loss: 2.1664, Perplexity: 8.72697\n",
      "Epoch [6/10], Step [8000/12942], Loss: 1.8105, Perplexity: 6.11360\n",
      "Epoch [6/10], Step [8100/12942], Loss: 2.2009, Perplexity: 9.03322\n",
      "Epoch [6/10], Step [8200/12942], Loss: 2.2488, Perplexity: 9.47650\n",
      "Epoch [6/10], Step [8300/12942], Loss: 2.3085, Perplexity: 10.0596\n",
      "Epoch [6/10], Step [8400/12942], Loss: 2.4137, Perplexity: 11.1748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [8500/12942], Loss: 1.9078, Perplexity: 6.73842\n",
      "Epoch [6/10], Step [8600/12942], Loss: 2.5375, Perplexity: 12.6482\n",
      "Epoch [6/10], Step [8700/12942], Loss: 2.0051, Perplexity: 7.42718\n",
      "Epoch [6/10], Step [8800/12942], Loss: 1.9192, Perplexity: 6.81572\n",
      "Epoch [6/10], Step [8900/12942], Loss: 2.5365, Perplexity: 12.6354\n",
      "Epoch [6/10], Step [9000/12942], Loss: 2.1204, Perplexity: 8.33457\n",
      "Epoch [6/10], Step [9100/12942], Loss: 2.0195, Perplexity: 7.53491\n",
      "Epoch [6/10], Step [9200/12942], Loss: 1.8601, Perplexity: 6.42436\n",
      "Epoch [6/10], Step [9300/12942], Loss: 2.3877, Perplexity: 10.8881\n",
      "Epoch [6/10], Step [9400/12942], Loss: 2.0701, Perplexity: 7.92563\n",
      "Epoch [6/10], Step [9500/12942], Loss: 2.0374, Perplexity: 7.67055\n",
      "Epoch [6/10], Step [9600/12942], Loss: 2.1042, Perplexity: 8.20041\n",
      "Epoch [6/10], Step [9700/12942], Loss: 2.4447, Perplexity: 11.5269\n",
      "Epoch [6/10], Step [9800/12942], Loss: 2.0400, Perplexity: 7.69095\n",
      "Epoch [6/10], Step [9900/12942], Loss: 2.2947, Perplexity: 9.92112\n",
      "Epoch [6/10], Step [10000/12942], Loss: 2.0804, Perplexity: 8.0075\n",
      "Epoch [6/10], Step [10100/12942], Loss: 2.2812, Perplexity: 9.78844\n",
      "Epoch [6/10], Step [10200/12942], Loss: 1.8280, Perplexity: 6.22143\n",
      "Epoch [6/10], Step [10300/12942], Loss: 2.2835, Perplexity: 9.81138\n",
      "Epoch [6/10], Step [10400/12942], Loss: 2.2527, Perplexity: 9.51334\n",
      "Epoch [6/10], Step [10500/12942], Loss: 2.7989, Perplexity: 16.4272\n",
      "Epoch [6/10], Step [10600/12942], Loss: 2.2859, Perplexity: 9.83434\n",
      "Epoch [6/10], Step [10700/12942], Loss: 2.4397, Perplexity: 11.4695\n",
      "Epoch [6/10], Step [10800/12942], Loss: 2.3736, Perplexity: 10.7355\n",
      "Epoch [6/10], Step [10900/12942], Loss: 2.2691, Perplexity: 9.67030\n",
      "Epoch [6/10], Step [11000/12942], Loss: 2.2329, Perplexity: 9.32645\n",
      "Epoch [6/10], Step [11100/12942], Loss: 2.0097, Perplexity: 7.46144\n",
      "Epoch [6/10], Step [11200/12942], Loss: 2.5393, Perplexity: 12.6710\n",
      "Epoch [6/10], Step [11300/12942], Loss: 2.4016, Perplexity: 11.0408\n",
      "Epoch [6/10], Step [11400/12942], Loss: 2.3935, Perplexity: 10.9519\n",
      "Epoch [6/10], Step [11500/12942], Loss: 2.2847, Perplexity: 9.82243\n",
      "Epoch [6/10], Step [11600/12942], Loss: 2.3732, Perplexity: 10.7312\n",
      "Epoch [6/10], Step [11700/12942], Loss: 2.1246, Perplexity: 8.36992\n",
      "Epoch [6/10], Step [11800/12942], Loss: 2.0891, Perplexity: 8.07731\n",
      "Epoch [6/10], Step [11900/12942], Loss: 2.1264, Perplexity: 8.38445\n",
      "Epoch [6/10], Step [12000/12942], Loss: 2.1322, Perplexity: 8.43327\n",
      "Epoch [6/10], Step [12100/12942], Loss: 2.0361, Perplexity: 7.66044\n",
      "Epoch [6/10], Step [12200/12942], Loss: 2.2104, Perplexity: 9.11903\n",
      "Epoch [6/10], Step [12300/12942], Loss: 1.9458, Perplexity: 6.99910\n",
      "Epoch [6/10], Step [12400/12942], Loss: 2.4592, Perplexity: 11.6959\n",
      "Epoch [6/10], Step [12500/12942], Loss: 2.1295, Perplexity: 8.41035\n",
      "Epoch [6/10], Step [12600/12942], Loss: 2.2195, Perplexity: 9.20297\n",
      "Epoch [6/10], Step [12700/12942], Loss: 1.9903, Perplexity: 7.31744\n",
      "Epoch [6/10], Step [12800/12942], Loss: 1.9435, Perplexity: 6.98323\n",
      "Epoch [6/10], Step [12900/12942], Loss: 2.0405, Perplexity: 7.69448\n",
      "Epoch [7/10], Step [100/12942], Loss: 1.9394, Perplexity: 6.9545454\n",
      "Epoch [7/10], Step [200/12942], Loss: 2.0155, Perplexity: 7.50424\n",
      "Epoch [7/10], Step [300/12942], Loss: 2.1215, Perplexity: 8.34375\n",
      "Epoch [7/10], Step [400/12942], Loss: 2.7058, Perplexity: 14.9669\n",
      "Epoch [7/10], Step [500/12942], Loss: 2.0668, Perplexity: 7.89969\n",
      "Epoch [7/10], Step [600/12942], Loss: 2.1349, Perplexity: 8.45593\n",
      "Epoch [7/10], Step [700/12942], Loss: 2.2838, Perplexity: 9.81414\n",
      "Epoch [7/10], Step [800/12942], Loss: 2.0458, Perplexity: 7.73501\n",
      "Epoch [7/10], Step [900/12942], Loss: 2.2697, Perplexity: 9.67602\n",
      "Epoch [7/10], Step [1000/12942], Loss: 2.2800, Perplexity: 9.7766\n",
      "Epoch [7/10], Step [1100/12942], Loss: 2.2743, Perplexity: 9.72137\n",
      "Epoch [7/10], Step [1200/12942], Loss: 2.0804, Perplexity: 8.00748\n",
      "Epoch [7/10], Step [1300/12942], Loss: 2.1519, Perplexity: 8.60119\n",
      "Epoch [7/10], Step [1400/12942], Loss: 2.1743, Perplexity: 8.79597\n",
      "Epoch [7/10], Step [1500/12942], Loss: 2.2204, Perplexity: 9.21086\n",
      "Epoch [7/10], Step [1600/12942], Loss: 1.7999, Perplexity: 6.04898\n",
      "Epoch [7/10], Step [1700/12942], Loss: 1.9822, Perplexity: 7.25856\n",
      "Epoch [7/10], Step [1800/12942], Loss: 2.1395, Perplexity: 8.49488\n",
      "Epoch [7/10], Step [1900/12942], Loss: 1.9394, Perplexity: 6.95441\n",
      "Epoch [7/10], Step [2000/12942], Loss: 1.9263, Perplexity: 6.86387\n",
      "Epoch [7/10], Step [2100/12942], Loss: 2.0709, Perplexity: 7.93215\n",
      "Epoch [7/10], Step [2200/12942], Loss: 1.9788, Perplexity: 7.23428\n",
      "Epoch [7/10], Step [2300/12942], Loss: 2.2521, Perplexity: 9.50773\n",
      "Epoch [7/10], Step [2400/12942], Loss: 2.3955, Perplexity: 10.9735\n",
      "Epoch [7/10], Step [2500/12942], Loss: 1.7818, Perplexity: 5.94074\n",
      "Epoch [7/10], Step [2600/12942], Loss: 1.9770, Perplexity: 7.22086\n",
      "Epoch [7/10], Step [2700/12942], Loss: 2.0141, Perplexity: 7.49406\n",
      "Epoch [7/10], Step [2800/12942], Loss: 2.2732, Perplexity: 9.71082\n",
      "Epoch [7/10], Step [2900/12942], Loss: 2.3184, Perplexity: 10.1599\n",
      "Epoch [7/10], Step [3000/12942], Loss: 2.3203, Perplexity: 10.1787\n",
      "Epoch [7/10], Step [3100/12942], Loss: 2.3913, Perplexity: 10.9274\n",
      "Epoch [7/10], Step [3200/12942], Loss: 2.1318, Perplexity: 8.43011\n",
      "Epoch [7/10], Step [3300/12942], Loss: 2.2731, Perplexity: 9.70966\n",
      "Epoch [7/10], Step [3400/12942], Loss: 2.2309, Perplexity: 9.30850\n",
      "Epoch [7/10], Step [3500/12942], Loss: 1.7963, Perplexity: 6.02714\n",
      "Epoch [7/10], Step [3600/12942], Loss: 2.2933, Perplexity: 9.90733\n",
      "Epoch [7/10], Step [3700/12942], Loss: 2.1679, Perplexity: 8.74001\n",
      "Epoch [7/10], Step [3800/12942], Loss: 2.0243, Perplexity: 7.57114\n",
      "Epoch [7/10], Step [3900/12942], Loss: 2.5182, Perplexity: 12.4058\n",
      "Epoch [7/10], Step [4000/12942], Loss: 2.2579, Perplexity: 9.56320\n",
      "Epoch [7/10], Step [4100/12942], Loss: 1.9458, Perplexity: 6.99929\n",
      "Epoch [7/10], Step [4200/12942], Loss: 2.8737, Perplexity: 17.7025\n",
      "Epoch [7/10], Step [4300/12942], Loss: 1.8848, Perplexity: 6.58533\n",
      "Epoch [7/10], Step [4400/12942], Loss: 1.8648, Perplexity: 6.45487\n",
      "Epoch [7/10], Step [4500/12942], Loss: 2.0300, Perplexity: 7.61398\n",
      "Epoch [7/10], Step [4600/12942], Loss: 2.1054, Perplexity: 8.21050\n",
      "Epoch [7/10], Step [4700/12942], Loss: 2.2152, Perplexity: 9.16335\n",
      "Epoch [7/10], Step [4800/12942], Loss: 1.9744, Perplexity: 7.20238\n",
      "Epoch [7/10], Step [4900/12942], Loss: 2.1191, Perplexity: 8.32350\n",
      "Epoch [7/10], Step [5000/12942], Loss: 2.0086, Perplexity: 7.453045\n",
      "Epoch [7/10], Step [5100/12942], Loss: 1.9414, Perplexity: 6.96887\n",
      "Epoch [7/10], Step [5200/12942], Loss: 2.0790, Perplexity: 7.99684\n",
      "Epoch [7/10], Step [5300/12942], Loss: 2.2052, Perplexity: 9.07234\n",
      "Epoch [7/10], Step [5400/12942], Loss: 2.0924, Perplexity: 8.10449\n",
      "Epoch [7/10], Step [5500/12942], Loss: 1.9536, Perplexity: 7.05397\n",
      "Epoch [7/10], Step [5600/12942], Loss: 2.1832, Perplexity: 8.87437\n",
      "Epoch [7/10], Step [5700/12942], Loss: 2.1420, Perplexity: 8.51646\n",
      "Epoch [7/10], Step [5800/12942], Loss: 2.0446, Perplexity: 7.72635\n",
      "Epoch [7/10], Step [5900/12942], Loss: 2.4254, Perplexity: 11.3072\n",
      "Epoch [7/10], Step [6000/12942], Loss: 1.9888, Perplexity: 7.30709\n",
      "Epoch [7/10], Step [6100/12942], Loss: 2.2562, Perplexity: 9.54651\n",
      "Epoch [7/10], Step [6200/12942], Loss: 2.5399, Perplexity: 12.6783\n",
      "Epoch [7/10], Step [6300/12942], Loss: 1.8942, Perplexity: 6.64750\n",
      "Epoch [7/10], Step [6400/12942], Loss: 2.6610, Perplexity: 14.3104\n",
      "Epoch [7/10], Step [6500/12942], Loss: 2.1578, Perplexity: 8.65237\n",
      "Epoch [7/10], Step [6600/12942], Loss: 2.1159, Perplexity: 8.29670\n",
      "Epoch [7/10], Step [6700/12942], Loss: 1.9180, Perplexity: 6.80751\n",
      "Epoch [7/10], Step [6800/12942], Loss: 1.9007, Perplexity: 6.69092\n",
      "Epoch [7/10], Step [6900/12942], Loss: 2.0434, Perplexity: 7.717080\n",
      "Epoch [7/10], Step [7000/12942], Loss: 2.2111, Perplexity: 9.12624\n",
      "Epoch [7/10], Step [7100/12942], Loss: 2.1974, Perplexity: 9.00128\n",
      "Epoch [7/10], Step [7200/12942], Loss: 2.1187, Perplexity: 8.32067\n",
      "Epoch [7/10], Step [7300/12942], Loss: 1.8550, Perplexity: 6.39194\n",
      "Epoch [7/10], Step [7400/12942], Loss: 2.7228, Perplexity: 15.2230\n",
      "Epoch [7/10], Step [7500/12942], Loss: 2.4745, Perplexity: 11.8759\n",
      "Epoch [7/10], Step [7600/12942], Loss: 2.0447, Perplexity: 7.72688\n",
      "Epoch [7/10], Step [7700/12942], Loss: 1.9959, Perplexity: 7.35883\n",
      "Epoch [7/10], Step [7800/12942], Loss: 2.1136, Perplexity: 8.27830\n",
      "Epoch [7/10], Step [7900/12942], Loss: 1.9248, Perplexity: 6.85360\n",
      "Epoch [7/10], Step [8000/12942], Loss: 2.0818, Perplexity: 8.01907\n",
      "Epoch [7/10], Step [8100/12942], Loss: 1.8150, Perplexity: 6.14091\n",
      "Epoch [7/10], Step [8200/12942], Loss: 2.0392, Perplexity: 7.68438\n",
      "Epoch [7/10], Step [8300/12942], Loss: 2.2677, Perplexity: 9.65682\n",
      "Epoch [7/10], Step [8400/12942], Loss: 2.0674, Perplexity: 7.90458\n",
      "Epoch [7/10], Step [8500/12942], Loss: 2.0470, Perplexity: 7.74452\n",
      "Epoch [7/10], Step [8600/12942], Loss: 1.7163, Perplexity: 5.56382\n",
      "Epoch [7/10], Step [8700/12942], Loss: 2.1130, Perplexity: 8.27307\n",
      "Epoch [7/10], Step [8800/12942], Loss: 1.9317, Perplexity: 6.90159\n",
      "Epoch [7/10], Step [8900/12942], Loss: 1.9217, Perplexity: 6.83296\n",
      "Epoch [7/10], Step [9000/12942], Loss: 2.1806, Perplexity: 8.85205\n",
      "Epoch [7/10], Step [9100/12942], Loss: 1.9186, Perplexity: 6.81154\n",
      "Epoch [7/10], Step [9200/12942], Loss: 2.4279, Perplexity: 11.3347\n",
      "Epoch [7/10], Step [9300/12942], Loss: 1.8311, Perplexity: 6.24057\n",
      "Epoch [7/10], Step [9400/12942], Loss: 2.0401, Perplexity: 7.69169\n",
      "Epoch [7/10], Step [9500/12942], Loss: 2.5565, Perplexity: 12.8910\n",
      "Epoch [7/10], Step [9600/12942], Loss: 1.9641, Perplexity: 7.12838\n",
      "Epoch [7/10], Step [9700/12942], Loss: 2.0592, Perplexity: 7.83963\n",
      "Epoch [7/10], Step [9800/12942], Loss: 2.0821, Perplexity: 8.02147\n",
      "Epoch [7/10], Step [9900/12942], Loss: 1.9930, Perplexity: 7.33731\n",
      "Epoch [7/10], Step [10000/12942], Loss: 2.0835, Perplexity: 8.0324\n",
      "Epoch [7/10], Step [10100/12942], Loss: 2.0794, Perplexity: 7.99948\n",
      "Epoch [7/10], Step [10200/12942], Loss: 2.6933, Perplexity: 14.7804\n",
      "Epoch [7/10], Step [10300/12942], Loss: 2.2135, Perplexity: 9.14759\n",
      "Epoch [7/10], Step [10400/12942], Loss: 1.9740, Perplexity: 7.19933\n",
      "Epoch [7/10], Step [10500/12942], Loss: 1.9685, Perplexity: 7.15980\n",
      "Epoch [7/10], Step [10600/12942], Loss: 2.7245, Perplexity: 15.2485\n",
      "Epoch [7/10], Step [10700/12942], Loss: 2.0908, Perplexity: 8.09131\n",
      "Epoch [7/10], Step [10800/12942], Loss: 2.1323, Perplexity: 8.43453\n",
      "Epoch [7/10], Step [10900/12942], Loss: 1.9876, Perplexity: 7.29826\n",
      "Epoch [7/10], Step [11000/12942], Loss: 3.0997, Perplexity: 22.1914\n",
      "Epoch [7/10], Step [11100/12942], Loss: 2.0212, Perplexity: 7.54723\n",
      "Epoch [7/10], Step [11200/12942], Loss: 1.8895, Perplexity: 6.61583\n",
      "Epoch [7/10], Step [11300/12942], Loss: 2.0667, Perplexity: 7.89869\n",
      "Epoch [7/10], Step [11400/12942], Loss: 2.3405, Perplexity: 10.3867\n",
      "Epoch [7/10], Step [11500/12942], Loss: 1.9769, Perplexity: 7.22008\n",
      "Epoch [7/10], Step [11600/12942], Loss: 2.3148, Perplexity: 10.1233\n",
      "Epoch [7/10], Step [11700/12942], Loss: 2.0578, Perplexity: 7.82876\n",
      "Epoch [7/10], Step [11800/12942], Loss: 2.0491, Perplexity: 7.76075\n",
      "Epoch [7/10], Step [11900/12942], Loss: 1.8916, Perplexity: 6.62996\n",
      "Epoch [7/10], Step [12000/12942], Loss: 2.5077, Perplexity: 12.2763\n",
      "Epoch [7/10], Step [12100/12942], Loss: 1.8621, Perplexity: 6.43719\n",
      "Epoch [7/10], Step [12200/12942], Loss: 1.8171, Perplexity: 6.15388\n",
      "Epoch [7/10], Step [12300/12942], Loss: 1.9304, Perplexity: 6.89204\n",
      "Epoch [7/10], Step [12400/12942], Loss: 2.1552, Perplexity: 8.62990\n",
      "Epoch [7/10], Step [12500/12942], Loss: 1.8912, Perplexity: 6.62760\n",
      "Epoch [7/10], Step [12600/12942], Loss: 2.0530, Perplexity: 7.79103\n",
      "Epoch [7/10], Step [12700/12942], Loss: 2.2413, Perplexity: 9.40595\n",
      "Epoch [7/10], Step [12800/12942], Loss: 2.2131, Perplexity: 9.14372\n",
      "Epoch [7/10], Step [12900/12942], Loss: 2.2473, Perplexity: 9.46207\n",
      "Epoch [8/10], Step [100/12942], Loss: 1.8300, Perplexity: 6.2341561\n",
      "Epoch [8/10], Step [200/12942], Loss: 2.0481, Perplexity: 7.75355\n",
      "Epoch [8/10], Step [300/12942], Loss: 2.5744, Perplexity: 13.1229\n",
      "Epoch [8/10], Step [400/12942], Loss: 2.2382, Perplexity: 9.37683\n",
      "Epoch [8/10], Step [500/12942], Loss: 2.2795, Perplexity: 9.77190\n",
      "Epoch [8/10], Step [600/12942], Loss: 1.8856, Perplexity: 6.59013\n",
      "Epoch [8/10], Step [700/12942], Loss: 2.1384, Perplexity: 8.48623\n",
      "Epoch [8/10], Step [800/12942], Loss: 2.3171, Perplexity: 10.1461\n",
      "Epoch [8/10], Step [900/12942], Loss: 2.1865, Perplexity: 8.90441\n",
      "Epoch [8/10], Step [1000/12942], Loss: 2.2348, Perplexity: 9.3449\n",
      "Epoch [8/10], Step [1100/12942], Loss: 2.0603, Perplexity: 7.84814\n",
      "Epoch [8/10], Step [1200/12942], Loss: 1.8666, Perplexity: 6.46667\n",
      "Epoch [8/10], Step [1300/12942], Loss: 2.1450, Perplexity: 8.54246\n",
      "Epoch [8/10], Step [1400/12942], Loss: 1.9802, Perplexity: 7.24418\n",
      "Epoch [8/10], Step [1500/12942], Loss: 2.3678, Perplexity: 10.6740\n",
      "Epoch [8/10], Step [1600/12942], Loss: 1.9232, Perplexity: 6.84294\n",
      "Epoch [8/10], Step [1700/12942], Loss: 2.2322, Perplexity: 9.32076\n",
      "Epoch [8/10], Step [1800/12942], Loss: 2.1544, Perplexity: 8.62246\n",
      "Epoch [8/10], Step [1900/12942], Loss: 2.1049, Perplexity: 8.20615\n",
      "Epoch [8/10], Step [2000/12942], Loss: 1.7901, Perplexity: 5.98990\n",
      "Epoch [8/10], Step [2100/12942], Loss: 2.2617, Perplexity: 9.59988\n",
      "Epoch [8/10], Step [2200/12942], Loss: 2.3914, Perplexity: 10.9293\n",
      "Epoch [8/10], Step [2300/12942], Loss: 2.0458, Perplexity: 7.73551\n",
      "Epoch [8/10], Step [2400/12942], Loss: 2.3608, Perplexity: 10.5992\n",
      "Epoch [8/10], Step [2500/12942], Loss: 1.8584, Perplexity: 6.41349\n",
      "Epoch [8/10], Step [2600/12942], Loss: 1.8946, Perplexity: 6.64987\n",
      "Epoch [8/10], Step [2700/12942], Loss: 2.0441, Perplexity: 7.72208\n",
      "Epoch [8/10], Step [2800/12942], Loss: 1.9160, Perplexity: 6.79360\n",
      "Epoch [8/10], Step [2900/12942], Loss: 2.1273, Perplexity: 8.39190\n",
      "Epoch [8/10], Step [3000/12942], Loss: 1.7103, Perplexity: 5.53040\n",
      "Epoch [8/10], Step [3100/12942], Loss: 2.1570, Perplexity: 8.64515\n",
      "Epoch [8/10], Step [3200/12942], Loss: 2.1946, Perplexity: 8.97654\n",
      "Epoch [8/10], Step [3300/12942], Loss: 1.8484, Perplexity: 6.34968\n",
      "Epoch [8/10], Step [3400/12942], Loss: 1.9307, Perplexity: 6.89404\n",
      "Epoch [8/10], Step [3500/12942], Loss: 2.0391, Perplexity: 7.68400\n",
      "Epoch [8/10], Step [3600/12942], Loss: 2.0059, Perplexity: 7.43305\n",
      "Epoch [8/10], Step [3700/12942], Loss: 2.2018, Perplexity: 9.04131\n",
      "Epoch [8/10], Step [3800/12942], Loss: 2.0992, Perplexity: 8.15970\n",
      "Epoch [8/10], Step [3900/12942], Loss: 2.4548, Perplexity: 11.6439\n",
      "Epoch [8/10], Step [4000/12942], Loss: 2.0956, Perplexity: 8.13077\n",
      "Epoch [8/10], Step [4100/12942], Loss: 1.7452, Perplexity: 5.72701\n",
      "Epoch [8/10], Step [4200/12942], Loss: 2.3318, Perplexity: 10.2968\n",
      "Epoch [8/10], Step [4300/12942], Loss: 2.1540, Perplexity: 8.61900\n",
      "Epoch [8/10], Step [4400/12942], Loss: 2.0185, Perplexity: 7.52687\n",
      "Epoch [8/10], Step [4500/12942], Loss: 1.7883, Perplexity: 5.97923\n",
      "Epoch [8/10], Step [4600/12942], Loss: 1.9613, Perplexity: 7.10837\n",
      "Epoch [8/10], Step [4700/12942], Loss: 2.4908, Perplexity: 12.0715\n",
      "Epoch [8/10], Step [4800/12942], Loss: 2.1332, Perplexity: 8.44165\n",
      "Epoch [8/10], Step [4900/12942], Loss: 1.9724, Perplexity: 7.18797\n",
      "Epoch [8/10], Step [5000/12942], Loss: 2.1166, Perplexity: 8.30338\n",
      "Epoch [8/10], Step [5100/12942], Loss: 3.0052, Perplexity: 20.1893\n",
      "Epoch [8/10], Step [5200/12942], Loss: 1.7257, Perplexity: 5.61664\n",
      "Epoch [8/10], Step [5300/12942], Loss: 2.0362, Perplexity: 7.66149\n",
      "Epoch [8/10], Step [5400/12942], Loss: 2.0848, Perplexity: 8.04303\n",
      "Epoch [8/10], Step [5500/12942], Loss: 2.0459, Perplexity: 7.73625\n",
      "Epoch [8/10], Step [5600/12942], Loss: 2.0304, Perplexity: 7.61691\n",
      "Epoch [8/10], Step [5700/12942], Loss: 2.1666, Perplexity: 8.72865\n",
      "Epoch [8/10], Step [5800/12942], Loss: 2.0909, Perplexity: 8.09222\n",
      "Epoch [8/10], Step [5900/12942], Loss: 2.0223, Perplexity: 7.55547\n",
      "Epoch [8/10], Step [6000/12942], Loss: 2.3240, Perplexity: 10.2165\n",
      "Epoch [8/10], Step [6100/12942], Loss: 2.7358, Perplexity: 15.4227\n",
      "Epoch [8/10], Step [6200/12942], Loss: 2.0114, Perplexity: 7.47353\n",
      "Epoch [8/10], Step [6300/12942], Loss: 2.3447, Perplexity: 10.4297\n",
      "Epoch [8/10], Step [6400/12942], Loss: 1.9159, Perplexity: 6.79293\n",
      "Epoch [8/10], Step [6500/12942], Loss: 2.0645, Perplexity: 7.88112\n",
      "Epoch [8/10], Step [6600/12942], Loss: 1.8748, Perplexity: 6.51986\n",
      "Epoch [8/10], Step [6700/12942], Loss: 1.9379, Perplexity: 6.94428\n",
      "Epoch [8/10], Step [6800/12942], Loss: 2.1678, Perplexity: 8.73908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [6900/12942], Loss: 1.8613, Perplexity: 6.43220\n",
      "Epoch [8/10], Step [7000/12942], Loss: 2.1885, Perplexity: 8.92155\n",
      "Epoch [8/10], Step [7100/12942], Loss: 1.8373, Perplexity: 6.27933\n",
      "Epoch [8/10], Step [7200/12942], Loss: 2.0605, Perplexity: 7.85029\n",
      "Epoch [8/10], Step [7300/12942], Loss: 2.0627, Perplexity: 7.86758\n",
      "Epoch [8/10], Step [7400/12942], Loss: 1.7088, Perplexity: 5.52252\n",
      "Epoch [8/10], Step [7500/12942], Loss: 1.9174, Perplexity: 6.80309\n",
      "Epoch [8/10], Step [7600/12942], Loss: 1.9285, Perplexity: 6.87891\n",
      "Epoch [8/10], Step [7700/12942], Loss: 2.1947, Perplexity: 8.97773\n",
      "Epoch [8/10], Step [7800/12942], Loss: 2.2011, Perplexity: 9.03533\n",
      "Epoch [8/10], Step [7900/12942], Loss: 2.2119, Perplexity: 9.13295\n",
      "Epoch [8/10], Step [8000/12942], Loss: 2.0483, Perplexity: 7.75512\n",
      "Epoch [8/10], Step [8100/12942], Loss: 1.8893, Perplexity: 6.61517\n",
      "Epoch [8/10], Step [8200/12942], Loss: 2.1489, Perplexity: 8.57508\n",
      "Epoch [8/10], Step [8300/12942], Loss: 2.4882, Perplexity: 12.0399\n",
      "Epoch [8/10], Step [8400/12942], Loss: 2.1238, Perplexity: 8.36264\n",
      "Epoch [8/10], Step [8500/12942], Loss: 1.5539, Perplexity: 4.73009\n",
      "Epoch [8/10], Step [8600/12942], Loss: 2.1633, Perplexity: 8.69972\n",
      "Epoch [8/10], Step [8700/12942], Loss: 1.9512, Perplexity: 7.03717\n",
      "Epoch [8/10], Step [8800/12942], Loss: 2.2574, Perplexity: 9.55794\n",
      "Epoch [8/10], Step [8900/12942], Loss: 2.1669, Perplexity: 8.73082\n",
      "Epoch [8/10], Step [9000/12942], Loss: 1.9100, Perplexity: 6.75336\n",
      "Epoch [8/10], Step [9100/12942], Loss: 1.9704, Perplexity: 7.17330\n",
      "Epoch [8/10], Step [9200/12942], Loss: 2.4709, Perplexity: 11.8335\n",
      "Epoch [8/10], Step [9300/12942], Loss: 2.2482, Perplexity: 9.47100\n",
      "Epoch [8/10], Step [9400/12942], Loss: 2.2417, Perplexity: 9.40942\n",
      "Epoch [8/10], Step [9500/12942], Loss: 2.2268, Perplexity: 9.26994\n",
      "Epoch [8/10], Step [9600/12942], Loss: 1.9568, Perplexity: 7.07681\n",
      "Epoch [8/10], Step [9700/12942], Loss: 1.9671, Perplexity: 7.14999\n",
      "Epoch [8/10], Step [9800/12942], Loss: 2.0047, Perplexity: 7.42403\n",
      "Epoch [8/10], Step [9900/12942], Loss: 2.1532, Perplexity: 8.61281\n",
      "Epoch [8/10], Step [10000/12942], Loss: 2.6093, Perplexity: 13.5895\n",
      "Epoch [8/10], Step [10100/12942], Loss: 1.8645, Perplexity: 6.452488\n",
      "Epoch [8/10], Step [10200/12942], Loss: 1.9714, Perplexity: 7.18061\n",
      "Epoch [8/10], Step [10300/12942], Loss: 2.4137, Perplexity: 11.1753\n",
      "Epoch [8/10], Step [10400/12942], Loss: 1.9462, Perplexity: 7.00204\n",
      "Epoch [8/10], Step [10500/12942], Loss: 1.8091, Perplexity: 6.10505\n",
      "Epoch [8/10], Step [10600/12942], Loss: 1.8173, Perplexity: 6.15507\n",
      "Epoch [8/10], Step [10700/12942], Loss: 2.1554, Perplexity: 8.63128\n",
      "Epoch [8/10], Step [10800/12942], Loss: 2.3014, Perplexity: 9.98810\n",
      "Epoch [8/10], Step [10900/12942], Loss: 2.3716, Perplexity: 10.7150\n",
      "Epoch [8/10], Step [11000/12942], Loss: 1.9783, Perplexity: 7.23075\n",
      "Epoch [8/10], Step [11100/12942], Loss: 1.9950, Perplexity: 7.35246\n",
      "Epoch [8/10], Step [11200/12942], Loss: 2.6551, Perplexity: 14.2270\n",
      "Epoch [8/10], Step [11300/12942], Loss: 2.0415, Perplexity: 7.70215\n",
      "Epoch [8/10], Step [11400/12942], Loss: 1.8162, Perplexity: 6.14832\n",
      "Epoch [8/10], Step [11500/12942], Loss: 2.3123, Perplexity: 10.0978\n",
      "Epoch [8/10], Step [11600/12942], Loss: 2.2255, Perplexity: 9.25813\n",
      "Epoch [8/10], Step [11700/12942], Loss: 1.7575, Perplexity: 5.79794\n",
      "Epoch [8/10], Step [11800/12942], Loss: 2.0835, Perplexity: 8.03242\n",
      "Epoch [8/10], Step [11900/12942], Loss: 2.1017, Perplexity: 8.18037\n",
      "Epoch [8/10], Step [12000/12942], Loss: 2.5425, Perplexity: 12.7114\n",
      "Epoch [8/10], Step [12100/12942], Loss: 2.0791, Perplexity: 7.99693\n",
      "Epoch [8/10], Step [12200/12942], Loss: 2.3429, Perplexity: 10.4110\n",
      "Epoch [8/10], Step [12300/12942], Loss: 1.9581, Perplexity: 7.08617\n",
      "Epoch [8/10], Step [12400/12942], Loss: 2.1287, Perplexity: 8.40433\n",
      "Epoch [8/10], Step [12500/12942], Loss: 2.1196, Perplexity: 8.32786\n",
      "Epoch [8/10], Step [12600/12942], Loss: 2.3226, Perplexity: 10.2021\n",
      "Epoch [8/10], Step [12700/12942], Loss: 2.6837, Perplexity: 14.6397\n",
      "Epoch [8/10], Step [12800/12942], Loss: 2.0187, Perplexity: 7.52860\n",
      "Epoch [8/10], Step [12900/12942], Loss: 2.0311, Perplexity: 7.62217\n",
      "Epoch [9/10], Step [100/12942], Loss: 1.8807, Perplexity: 6.5582564\n",
      "Epoch [9/10], Step [200/12942], Loss: 2.1175, Perplexity: 8.31043\n",
      "Epoch [9/10], Step [300/12942], Loss: 1.8178, Perplexity: 6.15813\n",
      "Epoch [9/10], Step [400/12942], Loss: 1.8243, Perplexity: 6.19868\n",
      "Epoch [9/10], Step [500/12942], Loss: 1.9972, Perplexity: 7.36815\n",
      "Epoch [9/10], Step [600/12942], Loss: 2.3043, Perplexity: 10.0169\n",
      "Epoch [9/10], Step [700/12942], Loss: 2.3256, Perplexity: 10.2329\n",
      "Epoch [9/10], Step [800/12942], Loss: 1.8084, Perplexity: 6.10098\n",
      "Epoch [9/10], Step [900/12942], Loss: 2.3817, Perplexity: 10.8231\n",
      "Epoch [9/10], Step [1000/12942], Loss: 2.0238, Perplexity: 7.5670\n",
      "Epoch [9/10], Step [1100/12942], Loss: 1.9408, Perplexity: 6.96453\n",
      "Epoch [9/10], Step [1200/12942], Loss: 2.2781, Perplexity: 9.75821\n",
      "Epoch [9/10], Step [1300/12942], Loss: 1.8993, Perplexity: 6.68159\n",
      "Epoch [9/10], Step [1400/12942], Loss: 1.7662, Perplexity: 5.84872\n",
      "Epoch [9/10], Step [1500/12942], Loss: 2.0492, Perplexity: 7.76183\n",
      "Epoch [9/10], Step [1600/12942], Loss: 2.2945, Perplexity: 9.91937\n",
      "Epoch [9/10], Step [1700/12942], Loss: 2.0712, Perplexity: 7.93414\n",
      "Epoch [9/10], Step [1800/12942], Loss: 2.0015, Perplexity: 7.40034\n",
      "Epoch [9/10], Step [1900/12942], Loss: 1.9276, Perplexity: 6.87312\n",
      "Epoch [9/10], Step [2000/12942], Loss: 2.1404, Perplexity: 8.50246\n",
      "Epoch [9/10], Step [2100/12942], Loss: 1.8729, Perplexity: 6.50737\n",
      "Epoch [9/10], Step [2200/12942], Loss: 2.1298, Perplexity: 8.41314\n",
      "Epoch [9/10], Step [2300/12942], Loss: 2.0258, Perplexity: 7.58201\n",
      "Epoch [9/10], Step [2400/12942], Loss: 1.7815, Perplexity: 5.93903\n",
      "Epoch [9/10], Step [2500/12942], Loss: 2.3339, Perplexity: 10.3185\n",
      "Epoch [9/10], Step [2600/12942], Loss: 2.1607, Perplexity: 8.67683\n",
      "Epoch [9/10], Step [2700/12942], Loss: 1.8484, Perplexity: 6.34979\n",
      "Epoch [9/10], Step [2800/12942], Loss: 2.0790, Perplexity: 7.99642\n",
      "Epoch [9/10], Step [2900/12942], Loss: 1.9338, Perplexity: 6.91601\n",
      "Epoch [9/10], Step [3000/12942], Loss: 1.9772, Perplexity: 7.22289\n",
      "Epoch [9/10], Step [3100/12942], Loss: 1.9644, Perplexity: 7.13056\n",
      "Epoch [9/10], Step [3200/12942], Loss: 2.0708, Perplexity: 7.93152\n",
      "Epoch [9/10], Step [3300/12942], Loss: 2.7037, Perplexity: 14.9342\n",
      "Epoch [9/10], Step [3400/12942], Loss: 1.9266, Perplexity: 6.86613\n",
      "Epoch [9/10], Step [3500/12942], Loss: 2.2266, Perplexity: 9.26809\n",
      "Epoch [9/10], Step [3600/12942], Loss: 2.0311, Perplexity: 7.62287\n",
      "Epoch [9/10], Step [3700/12942], Loss: 2.1221, Perplexity: 8.34841\n",
      "Epoch [9/10], Step [3800/12942], Loss: 2.0479, Perplexity: 7.75182\n",
      "Epoch [9/10], Step [3900/12942], Loss: 2.1465, Perplexity: 8.55466\n",
      "Epoch [9/10], Step [4000/12942], Loss: 1.9773, Perplexity: 7.22340\n",
      "Epoch [9/10], Step [4100/12942], Loss: 2.2722, Perplexity: 9.70022\n",
      "Epoch [9/10], Step [4200/12942], Loss: 1.9727, Perplexity: 7.19044\n",
      "Epoch [9/10], Step [4300/12942], Loss: 2.6837, Perplexity: 14.6393\n",
      "Epoch [9/10], Step [4400/12942], Loss: 2.2146, Perplexity: 9.15730\n",
      "Epoch [9/10], Step [4500/12942], Loss: 2.4455, Perplexity: 11.5359\n",
      "Epoch [9/10], Step [4600/12942], Loss: 1.9368, Perplexity: 6.93637\n",
      "Epoch [9/10], Step [4700/12942], Loss: 1.9988, Perplexity: 7.38031\n",
      "Epoch [9/10], Step [4800/12942], Loss: 2.2725, Perplexity: 9.70378\n",
      "Epoch [9/10], Step [4900/12942], Loss: 2.0636, Perplexity: 7.87398\n",
      "Epoch [9/10], Step [5000/12942], Loss: 1.9204, Perplexity: 6.82346\n",
      "Epoch [9/10], Step [5100/12942], Loss: 2.0291, Perplexity: 7.60700\n",
      "Epoch [9/10], Step [5200/12942], Loss: 1.7026, Perplexity: 5.48807\n",
      "Epoch [9/10], Step [5300/12942], Loss: 2.0349, Perplexity: 7.65150\n",
      "Epoch [9/10], Step [5400/12942], Loss: 2.1238, Perplexity: 8.36278\n",
      "Epoch [9/10], Step [5500/12942], Loss: 2.1801, Perplexity: 8.84761\n",
      "Epoch [9/10], Step [5600/12942], Loss: 2.0985, Perplexity: 8.15377\n",
      "Epoch [9/10], Step [5700/12942], Loss: 2.1616, Perplexity: 8.68485\n",
      "Epoch [9/10], Step [5800/12942], Loss: 2.0192, Perplexity: 7.53256\n",
      "Epoch [9/10], Step [5900/12942], Loss: 2.1820, Perplexity: 8.86437\n",
      "Epoch [9/10], Step [6000/12942], Loss: 1.9345, Perplexity: 6.92067\n",
      "Epoch [9/10], Step [6100/12942], Loss: 1.8695, Perplexity: 6.48493\n",
      "Epoch [9/10], Step [6200/12942], Loss: 2.3158, Perplexity: 10.1333\n",
      "Epoch [9/10], Step [6300/12942], Loss: 2.0233, Perplexity: 7.56309\n",
      "Epoch [9/10], Step [6400/12942], Loss: 2.0848, Perplexity: 8.04317\n",
      "Epoch [9/10], Step [6500/12942], Loss: 2.0496, Perplexity: 7.76482\n",
      "Epoch [9/10], Step [6600/12942], Loss: 1.8424, Perplexity: 6.31140\n",
      "Epoch [9/10], Step [6700/12942], Loss: 1.9777, Perplexity: 7.22618\n",
      "Epoch [9/10], Step [6800/12942], Loss: 2.3176, Perplexity: 10.1514\n",
      "Epoch [9/10], Step [6900/12942], Loss: 1.9578, Perplexity: 7.08358\n",
      "Epoch [9/10], Step [7000/12942], Loss: 2.5559, Perplexity: 12.8833\n",
      "Epoch [9/10], Step [7100/12942], Loss: 1.8840, Perplexity: 6.58001\n",
      "Epoch [9/10], Step [7200/12942], Loss: 2.3267, Perplexity: 10.2444\n",
      "Epoch [9/10], Step [7300/12942], Loss: 1.6829, Perplexity: 5.38145\n",
      "Epoch [9/10], Step [7400/12942], Loss: 3.1064, Perplexity: 22.3398\n",
      "Epoch [9/10], Step [7500/12942], Loss: 1.8796, Perplexity: 6.55074\n",
      "Epoch [9/10], Step [7600/12942], Loss: 2.4981, Perplexity: 12.1591\n",
      "Epoch [9/10], Step [7700/12942], Loss: 2.4106, Perplexity: 11.1409\n",
      "Epoch [9/10], Step [7800/12942], Loss: 2.0876, Perplexity: 8.06569\n",
      "Epoch [9/10], Step [7900/12942], Loss: 2.2800, Perplexity: 9.77679\n",
      "Epoch [9/10], Step [8000/12942], Loss: 2.0048, Perplexity: 7.42437\n",
      "Epoch [9/10], Step [8100/12942], Loss: 2.1962, Perplexity: 8.99124\n",
      "Epoch [9/10], Step [8200/12942], Loss: 1.8678, Perplexity: 6.47383\n",
      "Epoch [9/10], Step [8300/12942], Loss: 2.1517, Perplexity: 8.59982\n",
      "Epoch [9/10], Step [8400/12942], Loss: 2.1512, Perplexity: 8.59523\n",
      "Epoch [9/10], Step [8500/12942], Loss: 2.1496, Perplexity: 8.58184\n",
      "Epoch [9/10], Step [8600/12942], Loss: 2.0900, Perplexity: 8.08504\n",
      "Epoch [9/10], Step [8700/12942], Loss: 2.0282, Perplexity: 7.60064\n",
      "Epoch [9/10], Step [8800/12942], Loss: 2.1924, Perplexity: 8.95656\n",
      "Epoch [9/10], Step [8900/12942], Loss: 1.9651, Perplexity: 7.13535\n",
      "Epoch [9/10], Step [9000/12942], Loss: 1.9649, Perplexity: 7.13420\n",
      "Epoch [9/10], Step [9100/12942], Loss: 2.4285, Perplexity: 11.3424\n",
      "Epoch [9/10], Step [9200/12942], Loss: 2.8312, Perplexity: 16.9651\n",
      "Epoch [9/10], Step [9300/12942], Loss: 2.4161, Perplexity: 11.2019\n",
      "Epoch [9/10], Step [9400/12942], Loss: 2.2184, Perplexity: 9.19282\n",
      "Epoch [9/10], Step [9500/12942], Loss: 2.5991, Perplexity: 13.4515\n",
      "Epoch [9/10], Step [9600/12942], Loss: 2.1079, Perplexity: 8.23129\n",
      "Epoch [9/10], Step [9700/12942], Loss: 1.7973, Perplexity: 6.03366\n",
      "Epoch [9/10], Step [9800/12942], Loss: 2.1734, Perplexity: 8.78776\n",
      "Epoch [9/10], Step [9900/12942], Loss: 1.9545, Perplexity: 7.06062\n",
      "Epoch [9/10], Step [10000/12942], Loss: 2.1201, Perplexity: 8.3319\n",
      "Epoch [9/10], Step [10100/12942], Loss: 2.0654, Perplexity: 7.88863\n",
      "Epoch [9/10], Step [10200/12942], Loss: 2.1419, Perplexity: 8.51597\n",
      "Epoch [9/10], Step [10300/12942], Loss: 2.4458, Perplexity: 11.5401\n",
      "Epoch [9/10], Step [10400/12942], Loss: 2.3355, Perplexity: 10.3348\n",
      "Epoch [9/10], Step [10500/12942], Loss: 2.0467, Perplexity: 7.74250\n",
      "Epoch [9/10], Step [10600/12942], Loss: 2.0714, Perplexity: 7.93622\n",
      "Epoch [9/10], Step [10700/12942], Loss: 2.2979, Perplexity: 9.95312\n",
      "Epoch [9/10], Step [10800/12942], Loss: 2.0512, Perplexity: 7.77709\n",
      "Epoch [9/10], Step [10900/12942], Loss: 2.1278, Perplexity: 8.39670\n",
      "Epoch [9/10], Step [11000/12942], Loss: 1.7382, Perplexity: 5.68727\n",
      "Epoch [9/10], Step [11100/12942], Loss: 2.0588, Perplexity: 7.83657\n",
      "Epoch [9/10], Step [11200/12942], Loss: 2.1958, Perplexity: 8.98705\n",
      "Epoch [9/10], Step [11300/12942], Loss: 1.8920, Perplexity: 6.63240\n",
      "Epoch [9/10], Step [11400/12942], Loss: 2.0217, Perplexity: 7.55145\n",
      "Epoch [9/10], Step [11500/12942], Loss: 2.2204, Perplexity: 9.21109\n",
      "Epoch [9/10], Step [11600/12942], Loss: 2.1098, Perplexity: 8.24660\n",
      "Epoch [9/10], Step [11700/12942], Loss: 2.1145, Perplexity: 8.28541\n",
      "Epoch [9/10], Step [11800/12942], Loss: 1.6909, Perplexity: 5.42435\n",
      "Epoch [9/10], Step [11900/12942], Loss: 2.1980, Perplexity: 9.00675\n",
      "Epoch [9/10], Step [12000/12942], Loss: 2.0356, Perplexity: 7.65706\n",
      "Epoch [9/10], Step [12100/12942], Loss: 2.3253, Perplexity: 10.2298\n",
      "Epoch [9/10], Step [12200/12942], Loss: 2.0124, Perplexity: 7.48102\n",
      "Epoch [9/10], Step [12300/12942], Loss: 2.3499, Perplexity: 10.4844\n",
      "Epoch [9/10], Step [12400/12942], Loss: 1.8601, Perplexity: 6.42422\n",
      "Epoch [9/10], Step [12500/12942], Loss: 1.9661, Perplexity: 7.14290\n",
      "Epoch [9/10], Step [12600/12942], Loss: 2.0796, Perplexity: 8.00105\n",
      "Epoch [9/10], Step [12700/12942], Loss: 2.0018, Perplexity: 7.40252\n",
      "Epoch [9/10], Step [12800/12942], Loss: 2.0239, Perplexity: 7.56740\n",
      "Epoch [9/10], Step [12900/12942], Loss: 1.9879, Perplexity: 7.30025\n",
      "Epoch [10/10], Step [100/12942], Loss: 2.0076, Perplexity: 7.445410\n",
      "Epoch [10/10], Step [200/12942], Loss: 2.4824, Perplexity: 11.9705\n",
      "Epoch [10/10], Step [300/12942], Loss: 2.2214, Perplexity: 9.22056\n",
      "Epoch [10/10], Step [400/12942], Loss: 1.9266, Perplexity: 6.86618\n",
      "Epoch [10/10], Step [500/12942], Loss: 2.0371, Perplexity: 7.66818\n",
      "Epoch [10/10], Step [600/12942], Loss: 2.1307, Perplexity: 8.42081\n",
      "Epoch [10/10], Step [700/12942], Loss: 2.5594, Perplexity: 12.9277\n",
      "Epoch [10/10], Step [800/12942], Loss: 1.8056, Perplexity: 6.08349\n",
      "Epoch [10/10], Step [900/12942], Loss: 2.0330, Perplexity: 7.63673\n",
      "Epoch [10/10], Step [1000/12942], Loss: 2.3134, Perplexity: 10.1092\n",
      "Epoch [10/10], Step [1100/12942], Loss: 2.2680, Perplexity: 9.66044\n",
      "Epoch [10/10], Step [1200/12942], Loss: 2.2154, Perplexity: 9.16482\n",
      "Epoch [10/10], Step [1300/12942], Loss: 2.0385, Perplexity: 7.67937\n",
      "Epoch [10/10], Step [1400/12942], Loss: 1.9142, Perplexity: 6.78142\n",
      "Epoch [10/10], Step [1500/12942], Loss: 2.0389, Perplexity: 7.68240\n",
      "Epoch [10/10], Step [1600/12942], Loss: 2.1104, Perplexity: 8.25150\n",
      "Epoch [10/10], Step [1700/12942], Loss: 1.9367, Perplexity: 6.93613\n",
      "Epoch [10/10], Step [1800/12942], Loss: 2.3799, Perplexity: 10.8033\n",
      "Epoch [10/10], Step [1900/12942], Loss: 1.8195, Perplexity: 6.16877\n",
      "Epoch [10/10], Step [2000/12942], Loss: 2.1361, Perplexity: 8.46645\n",
      "Epoch [10/10], Step [2100/12942], Loss: 2.0015, Perplexity: 7.40011\n",
      "Epoch [10/10], Step [2200/12942], Loss: 1.9409, Perplexity: 6.96543\n",
      "Epoch [10/10], Step [2300/12942], Loss: 2.1059, Perplexity: 8.21412\n",
      "Epoch [10/10], Step [2400/12942], Loss: 2.4663, Perplexity: 11.7782\n",
      "Epoch [10/10], Step [2500/12942], Loss: 2.1247, Perplexity: 8.37035\n",
      "Epoch [10/10], Step [2600/12942], Loss: 1.8796, Perplexity: 6.55100\n",
      "Epoch [10/10], Step [2700/12942], Loss: 2.2480, Perplexity: 9.46883\n",
      "Epoch [10/10], Step [2800/12942], Loss: 2.2137, Perplexity: 9.14968\n",
      "Epoch [10/10], Step [2900/12942], Loss: 1.6929, Perplexity: 5.43516\n",
      "Epoch [10/10], Step [3000/12942], Loss: 2.1393, Perplexity: 8.49357\n",
      "Epoch [10/10], Step [3100/12942], Loss: 2.1287, Perplexity: 8.40423\n",
      "Epoch [10/10], Step [3200/12942], Loss: 2.3732, Perplexity: 10.7319\n",
      "Epoch [10/10], Step [3300/12942], Loss: 2.0780, Perplexity: 7.98839\n",
      "Epoch [10/10], Step [3400/12942], Loss: 2.5761, Perplexity: 13.1457\n",
      "Epoch [10/10], Step [3500/12942], Loss: 2.0456, Perplexity: 7.73373\n",
      "Epoch [10/10], Step [3600/12942], Loss: 2.0797, Perplexity: 8.00188\n",
      "Epoch [10/10], Step [3700/12942], Loss: 2.3069, Perplexity: 10.0428\n",
      "Epoch [10/10], Step [3800/12942], Loss: 1.7771, Perplexity: 5.91280\n",
      "Epoch [10/10], Step [3900/12942], Loss: 2.8109, Perplexity: 16.6252\n",
      "Epoch [10/10], Step [4000/12942], Loss: 1.9438, Perplexity: 6.98544\n",
      "Epoch [10/10], Step [4100/12942], Loss: 1.9711, Perplexity: 7.17896\n",
      "Epoch [10/10], Step [4200/12942], Loss: 1.9134, Perplexity: 6.77625\n",
      "Epoch [10/10], Step [4300/12942], Loss: 2.3043, Perplexity: 10.0174\n",
      "Epoch [10/10], Step [4400/12942], Loss: 1.8705, Perplexity: 6.49131\n",
      "Epoch [10/10], Step [4500/12942], Loss: 2.1543, Perplexity: 8.62176\n",
      "Epoch [10/10], Step [4600/12942], Loss: 2.0017, Perplexity: 7.40143\n",
      "Epoch [10/10], Step [4700/12942], Loss: 1.9775, Perplexity: 7.22435\n",
      "Epoch [10/10], Step [4800/12942], Loss: 2.2471, Perplexity: 9.46053\n",
      "Epoch [10/10], Step [4900/12942], Loss: 2.0416, Perplexity: 7.70267\n",
      "Epoch [10/10], Step [5000/12942], Loss: 1.9440, Perplexity: 6.98652\n",
      "Epoch [10/10], Step [5100/12942], Loss: 2.2450, Perplexity: 9.44087\n",
      "Epoch [10/10], Step [5200/12942], Loss: 1.9629, Perplexity: 7.11972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [5300/12942], Loss: 2.0209, Perplexity: 7.54513\n",
      "Epoch [10/10], Step [5400/12942], Loss: 2.1191, Perplexity: 8.32346\n",
      "Epoch [10/10], Step [5500/12942], Loss: 2.1146, Perplexity: 8.28653\n",
      "Epoch [10/10], Step [5600/12942], Loss: 1.9655, Perplexity: 7.13859\n",
      "Epoch [10/10], Step [5700/12942], Loss: 2.1341, Perplexity: 8.44980\n",
      "Epoch [10/10], Step [5800/12942], Loss: 2.2284, Perplexity: 9.28530\n",
      "Epoch [10/10], Step [5900/12942], Loss: 2.5382, Perplexity: 12.6569\n",
      "Epoch [10/10], Step [6000/12942], Loss: 2.2623, Perplexity: 9.60566\n",
      "Epoch [10/10], Step [6100/12942], Loss: 1.9360, Perplexity: 6.93086\n",
      "Epoch [10/10], Step [6200/12942], Loss: 2.1702, Perplexity: 8.76047\n",
      "Epoch [10/10], Step [6300/12942], Loss: 2.4349, Perplexity: 11.4142\n",
      "Epoch [10/10], Step [6400/12942], Loss: 2.3649, Perplexity: 10.6432\n",
      "Epoch [10/10], Step [6500/12942], Loss: 2.1212, Perplexity: 8.34112\n",
      "Epoch [10/10], Step [6600/12942], Loss: 1.8315, Perplexity: 6.24318\n",
      "Epoch [10/10], Step [6700/12942], Loss: 1.7112, Perplexity: 5.53578\n",
      "Epoch [10/10], Step [6800/12942], Loss: 2.1109, Perplexity: 8.25564\n",
      "Epoch [10/10], Step [6900/12942], Loss: 2.0316, Perplexity: 7.62617\n",
      "Epoch [10/10], Step [7000/12942], Loss: 1.8082, Perplexity: 6.09938\n",
      "Epoch [10/10], Step [7100/12942], Loss: 1.8130, Perplexity: 6.12855\n",
      "Epoch [10/10], Step [7200/12942], Loss: 2.2599, Perplexity: 9.58182\n",
      "Epoch [10/10], Step [7300/12942], Loss: 1.7591, Perplexity: 5.80753\n",
      "Epoch [10/10], Step [7400/12942], Loss: 2.0317, Perplexity: 7.62685\n",
      "Epoch [10/10], Step [7500/12942], Loss: 1.9106, Perplexity: 6.75741\n",
      "Epoch [10/10], Step [7600/12942], Loss: 3.1670, Perplexity: 23.73721\n",
      "Epoch [10/10], Step [7700/12942], Loss: 2.0965, Perplexity: 8.13762\n",
      "Epoch [10/10], Step [7800/12942], Loss: 2.2674, Perplexity: 9.65389\n",
      "Epoch [10/10], Step [7900/12942], Loss: 2.1471, Perplexity: 8.55986\n",
      "Epoch [10/10], Step [8000/12942], Loss: 2.3933, Perplexity: 10.9491\n",
      "Epoch [10/10], Step [8100/12942], Loss: 1.9974, Perplexity: 7.37018\n",
      "Epoch [10/10], Step [8200/12942], Loss: 1.9190, Perplexity: 6.81438\n",
      "Epoch [10/10], Step [8300/12942], Loss: 2.0540, Perplexity: 7.79921\n",
      "Epoch [10/10], Step [8400/12942], Loss: 2.0688, Perplexity: 7.91510\n",
      "Epoch [10/10], Step [8500/12942], Loss: 2.2907, Perplexity: 9.88208\n",
      "Epoch [10/10], Step [8600/12942], Loss: 1.9502, Perplexity: 7.02984\n",
      "Epoch [10/10], Step [8700/12942], Loss: 1.9859, Perplexity: 7.28588\n",
      "Epoch [10/10], Step [8800/12942], Loss: 1.9081, Perplexity: 6.74049\n",
      "Epoch [10/10], Step [8900/12942], Loss: 2.1594, Perplexity: 8.66617\n",
      "Epoch [10/10], Step [9000/12942], Loss: 2.0163, Perplexity: 7.51061\n",
      "Epoch [10/10], Step [9100/12942], Loss: 2.0772, Perplexity: 7.98198\n",
      "Epoch [10/10], Step [9200/12942], Loss: 1.9045, Perplexity: 6.71586\n",
      "Epoch [10/10], Step [9300/12942], Loss: 1.8872, Perplexity: 6.60105\n",
      "Epoch [10/10], Step [9400/12942], Loss: 2.0876, Perplexity: 8.06521\n",
      "Epoch [10/10], Step [9500/12942], Loss: 1.9817, Perplexity: 7.25495\n",
      "Epoch [10/10], Step [9600/12942], Loss: 1.9357, Perplexity: 6.92918\n",
      "Epoch [10/10], Step [9700/12942], Loss: 1.9450, Perplexity: 6.99388\n",
      "Epoch [10/10], Step [9800/12942], Loss: 2.9212, Perplexity: 18.5634\n",
      "Epoch [10/10], Step [9900/12942], Loss: 2.0735, Perplexity: 7.95254\n",
      "Epoch [10/10], Step [10000/12942], Loss: 2.1688, Perplexity: 8.7482\n",
      "Epoch [10/10], Step [10100/12942], Loss: 1.9050, Perplexity: 6.71978\n",
      "Epoch [10/10], Step [10200/12942], Loss: 1.9112, Perplexity: 6.76139\n",
      "Epoch [10/10], Step [10300/12942], Loss: 2.1023, Perplexity: 8.18518\n",
      "Epoch [10/10], Step [10400/12942], Loss: 2.5623, Perplexity: 12.9653\n",
      "Epoch [10/10], Step [10500/12942], Loss: 2.3721, Perplexity: 10.7197\n",
      "Epoch [10/10], Step [10600/12942], Loss: 2.2591, Perplexity: 9.57425\n",
      "Epoch [10/10], Step [10700/12942], Loss: 2.2684, Perplexity: 9.66426\n",
      "Epoch [10/10], Step [10800/12942], Loss: 1.7524, Perplexity: 5.76875\n",
      "Epoch [10/10], Step [10900/12942], Loss: 2.2439, Perplexity: 9.42975\n",
      "Epoch [10/10], Step [11000/12942], Loss: 2.3602, Perplexity: 10.5931\n",
      "Epoch [10/10], Step [11100/12942], Loss: 2.0784, Perplexity: 7.99178\n",
      "Epoch [10/10], Step [11200/12942], Loss: 1.9606, Perplexity: 7.10371\n",
      "Epoch [10/10], Step [11300/12942], Loss: 2.1660, Perplexity: 8.72344\n",
      "Epoch [10/10], Step [11400/12942], Loss: 1.9958, Perplexity: 7.35835\n",
      "Epoch [10/10], Step [11500/12942], Loss: 1.9752, Perplexity: 7.20803\n",
      "Epoch [10/10], Step [11600/12942], Loss: 2.5164, Perplexity: 12.3837\n",
      "Epoch [10/10], Step [11700/12942], Loss: 2.0560, Perplexity: 7.81487\n",
      "Epoch [10/10], Step [11800/12942], Loss: 2.0430, Perplexity: 7.71414\n",
      "Epoch [10/10], Step [11900/12942], Loss: 2.0673, Perplexity: 7.90359\n",
      "Epoch [10/10], Step [12000/12942], Loss: 1.8739, Perplexity: 6.51388\n",
      "Epoch [10/10], Step [12100/12942], Loss: 1.9539, Perplexity: 7.05628\n",
      "Epoch [10/10], Step [12200/12942], Loss: 2.2820, Perplexity: 9.79599\n",
      "Epoch [10/10], Step [12300/12942], Loss: 1.8453, Perplexity: 6.33017\n",
      "Epoch [10/10], Step [12400/12942], Loss: 2.1567, Perplexity: 8.64223\n",
      "Epoch [10/10], Step [12500/12942], Loss: 2.0679, Perplexity: 7.90793\n",
      "Epoch [10/10], Step [12600/12942], Loss: 2.1523, Perplexity: 8.60437\n",
      "Epoch [10/10], Step [12700/12942], Loss: 1.9406, Perplexity: 6.96316\n",
      "Epoch [10/10], Step [12800/12942], Loss: 2.0260, Perplexity: 7.58381\n",
      "Epoch [10/10], Step [12900/12942], Loss: 1.9998, Perplexity: 7.38725\n",
      "Epoch [10/10], Step [12942/12942], Loss: 2.0919, Perplexity: 8.10002"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
